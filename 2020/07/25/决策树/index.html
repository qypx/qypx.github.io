<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,">










<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="http://qypx.github.io/2020/07/25/决策树/index.html">
<meta property="og:site_name" content="qypx の blog">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649657475.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650023450.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650557293.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650609697.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650752017.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650778439.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650963697.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595651148743.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649034648.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649090585.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649112772.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649135709.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595652338160.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595652444294.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595652870653.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595653661803.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595653718100.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601196695960.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663386859.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663456133.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663508910.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663617670.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667008154.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667099649.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667123278.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667175530.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601189631302.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601190228458.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601190289236.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601190443051.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601191484592.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669502617.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669541744.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667573239.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667719081.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667791401.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667837751.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667861205.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667888425.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667950419.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667972342.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667995025.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668020156.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595822346216.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668057942.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595822888787.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668137245.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668154967.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595823332714.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668243451.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668320876.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668340241.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668403168.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668446774.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668537184.png">
<meta property="og:image" content="data:image/png;base64,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">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668680481.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595830272543.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601268650680.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1601268679265.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595837055748.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669193083.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669223727.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669343135.png">
<meta property="og:updated_time" content="2020-09-28T05:06:57.181Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树">
<meta name="twitter:image" content="http://qypx.github.io/2020/07/25/决策树/1595649657475.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://qypx.github.io/2020/07/25/决策树/">





<!-- 网页加载条 -->
<script src="https://neveryu.github.io/js/src/pace.min.js"></script>

  <title>决策树 | qypx の blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">qypx の blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">机会是留给有准备的人的.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://qypx.github.io/2020/07/25/决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qypx">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="qypx の blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-25T13:48:42+08:00">
                2020-07-25
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2020-09-28T13:06:57+08:00">
                2020-09-28
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读量
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>（关于决策树与集成学习也可参考我的OneNote笔记 -&gt; preparation）</p>
<h2 id="篇零：决策树的分类"><a href="#篇零：决策树的分类" class="headerlink" title="篇零：决策树的分类"></a>篇零：决策树的分类</h2><blockquote>
<p>本部分内容来自 <a href="https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A</a></p>
</blockquote>
<p>决策树分为两大类：分类树和回归树。前者用于预测离散的具体类别：是否晴天/好坏/网页是否是垃圾网页等等，后者预测的是连续的具体的值：年龄/身高等等。前者的累加是没有意义的比如‘男+男+女=？’，而后者结果的加减是有意义的，‘10+3-6=7’。</p>
<p><strong>1.分类树</strong></p>
<p>决策树的分类算法有很多，以具有最大熵的特征进行分类，以信息增益特征进行分类（ID3），以增益率特征进行分类（C4.5），以基尼系数特征进行分类（CART分类与回归树）等等。这一类决策树的特点就是最后的结果都是离散的具体的类别，比如苹果的好/坏，性别男/女。</p>
<p><strong>2.回归树</strong></p>
<p>回归树与分类树的流程大致一样，不同的是回归树在每个节点都会有一个预测值，以年龄为例，该节点的预测值就是所有属于该节点的样本的年龄的均值。</p>
<p>那回归树是根据什么来划分特征的呢？</p>
<p>分类树的最大熵、信息增益、增益率什么的在回归树这都不适用了，回归树用的是均方误差。遍历每个特征，穷举每个特征的划分阈值，而这里不再使用最大熵，使用的是最小化均方差——(每个人的年龄-预测年龄)<em>^2/N，N</em>代表节点内样本数。这很好理解，和预测年龄差距越大，均方差也就越大。因此要找到均方差最小的阈值作为划分点。</p>
<p>划分的结束条件一般有两个：第一是划分到每一个节点都只包含一个年龄值，但是这太难了；第二就是划分到一定的深度就停止，取节点内数据的均值作为最终的预测值。</p>
<hr>
<h2 id="篇一：决策树-（ID3-C4-5-CART）"><a href="#篇一：决策树-（ID3-C4-5-CART）" class="headerlink" title="篇一：决策树 （ID3, C4.5, CART）"></a>篇一：决策树 （ID3, C4.5, CART）</h2><blockquote>
<p>本部分内容来源于公众号Datawhale</p>
<p><a href="https://mp.weixin.qq.com/s/jj3BtmnWRAwCS56ZU3ZXZA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/jj3BtmnWRAwCS56ZU3ZXZA</a></p>
</blockquote>
<h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><p>ID3 算法是建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。</p>
<h4 id="1-1-思想"><a href="#1-1-思想" class="headerlink" title="1.1 思想"></a>1.1 思想</h4><p>从信息论的知识中我们知道：期望信息越小，信息熵越大，从而样本纯度越低。ID3 算法的核心思想就是以信息增益来度量特征选择，<strong>选择<span style="color:red">信息增益</span>最大的特征进行分裂</strong>。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。</p>
<p>其大致步骤为：</p>
<ol>
<li>初始化特征集合和数据集合；</li>
<li>计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；</li>
<li>更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；</li>
<li>重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。</li>
</ol>
<h4 id="1-2-划分标准"><a href="#1-2-划分标准" class="headerlink" title="1.2 划分标准"></a>1.2 划分标准</h4><p>ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度。</p>
<p><img src="/2020/07/25/决策树/1595649657475.png" alt="1595649657475"></p>
<p><strong>信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</strong></p>
<h4 id="1-3-缺点"><a href="#1-3-缺点" class="headerlink" title="1.3 缺点"></a>1.3 缺点</h4><ul>
<li>ID3 没有剪枝策略，容易过拟合；</li>
<li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1（但实际上这并不是一个好的特征选择）；</li>
<li>只能用于处理离散分布的特征；</li>
<li>没有考虑缺失值。</li>
</ul>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p><strong>C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点</strong>，引入<strong><span style="color:red">信息增益率</span></strong>来作为分类标准。</p>
<h4 id="2-1-思想"><a href="#2-1-思想" class="headerlink" title="2.1 思想"></a>2.1 思想</h4><p>C4.5 相对于 ID3 的缺点对应有以下改进方式：</p>
<ul>
<li><p>引入悲观剪枝策略进行后剪枝；</p>
</li>
<li><p>引入信息增益率作为划分标准；</p>
</li>
<li><p>将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；</p>
</li>
<li><p>对于缺失值的处理可以分为两个子问题：1. 在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）2. 选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）</p>
</li>
<li><ul>
<li>针对问题一，C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；</li>
<li>针对问题二，C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</li>
</ul>
</li>
</ul>
<h4 id="2-2-划分标准"><a href="#2-2-划分标准" class="headerlink" title="2.2 划分标准"></a>2.2 划分标准</h4><p><img src="/2020/07/25/决策树/1595650023450.png" alt="1595650023450"></p>
<p>这里需要注意，信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个<em>启发式方法</em>：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p>
<h4 id="2-3-剪枝策略"><a href="#2-3-剪枝策略" class="headerlink" title="2.3 剪枝策略"></a>2.3 剪枝策略</h4><p>为什么要剪枝：过拟合的树在泛化能力的表现非常差。</p>
<h5 id="2-3-1-预剪枝"><a href="#2-3-1-预剪枝" class="headerlink" title="2.3.1 预剪枝"></a>2.3.1 预剪枝</h5><p>在节点划分前来确定是否继续增长，及早停止增长的主要方法有：</p>
<ul>
<li>节点内数据样本低于某一阈值；</li>
<li>所有节点特征都已分裂；</li>
<li>节点划分前准确率比划分后准确率高。</li>
</ul>
<p>预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。</p>
<h5 id="2-3-2-后剪枝"><a href="#2-3-2-后剪枝" class="headerlink" title="2.3.2 后剪枝"></a>2.3.2 后剪枝</h5><p>在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。</p>
<p><strong>C4.5 采用的悲观剪枝方法</strong>，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。</p>
<p>后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。</p>
<h4 id="2-4-缺点"><a href="#2-4-缺点" class="headerlink" title="2.4 缺点"></a>2.4 缺点</h4><ul>
<li>剪枝策略可以再优化；</li>
<li>C4.5 用的是多叉树，用二叉树效率更高；</li>
<li>C4.5 只能用于分类；</li>
<li>C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；</li>
<li>C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li>
</ul>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。</p>
<h4 id="3-1-思想"><a href="#3-1-思想" class="headerlink" title="3.1 思想"></a>3.1 思想</h4><p>CART 包含的基本过程有分裂，剪枝和树选择。</p>
<ul>
<li><strong>分裂</strong>：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；</li>
<li><strong>剪枝</strong>：<strong>采用<em>代价复杂度剪枝</em></strong>，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；</li>
<li><strong>树选择</strong>：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。</li>
</ul>
<p>CART 在 C4.5 的基础上进行了很多提升。</p>
<ul>
<li>C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；</li>
<li>C4.5 只能分类，CART 既可以分类也可以回归；</li>
<li>CART 使用 <strong><span style="color:red">Gini 系数</span></strong> 作为变量的不纯度量，减少了大量的对数运算；</li>
<li>CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；</li>
<li>CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。</li>
</ul>
<h4 id="3-2-划分标准"><a href="#3-2-划分标准" class="headerlink" title="3.2 划分标准"></a>3.2 划分标准</h4><p>熵模型拥有大量耗时的对数运算，基尼指数在简化模型的同时还保留了熵模型的优点。<strong>基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。</strong></p>
<p><img src="/2020/07/25/决策树/1595650557293.png" alt="1595650557293"></p>
<p><img src="/2020/07/25/决策树/1595650609697.png" alt="1595650609697"></p>
<h4 id="3-3-缺失值处理"><a href="#3-3-缺失值处理" class="headerlink" title="3.3 缺失值处理"></a>3.3 缺失值处理</h4><p>上文说到，模型对于缺失值的处理会分为两个子问题：1. 在特征值缺失的情况下进行划分特征的选择？2. 选定该划分特征，对于缺失该特征值的样本如何处理？</p>
<p>对于问题 1，CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。</p>
<p>对于问题 2，CART 算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。</p>
<h4 id="3-4-剪枝策略"><a href="#3-4-剪枝策略" class="headerlink" title="3.4 剪枝策略"></a>3.4 剪枝策略</h4><p>采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。</p>
<p>我们来看具体看一下代价复杂度剪枝算法：</p>
<p><img src="/2020/07/25/决策树/1595650752017.png" alt="1595650752017"></p>
<p><img src="/2020/07/25/决策树/1595650778439.png" alt="1595650778439"></p>
<h4 id="3-5-类别不平衡"><a href="#3-5-类别不平衡" class="headerlink" title="3.5 类别不平衡"></a>3.5 类别不平衡</h4><p>CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其自动消除不需要建模人员采取其他操作。</p>
<p>CART 使用了一种先验机制，其作用相当于对类别进行加权。这种先验机制嵌入于 CART 算法判断分裂优劣的运算里，在 CART 默认的分类模式中，总是要计算每个节点关于根节点的类别频率的比值，这就相当于对数据自动重加权，对类别进行均衡。</p>
<p><img src="/2020/07/25/决策树/1595650963697.png" alt="1595650963697"></p>
<p>通过这种计算方式就无需管理数据真实的类别分布。假设有 K 个目标类别，就可以确保根节点中每个类别的概率都是 1/K。这种默认的模式被称为“先验相等”。</p>
<p>先验设置和加权不同之处在于先验不影响每个节点中的各类别样本的数量或者份额。先验影响的是每个节点的类别赋值和树生长过程中分裂的选择。</p>
<h4 id="3-6-回归树"><a href="#3-6-回归树" class="headerlink" title="3.6 回归树"></a>3.6 回归树</h4><p>CART(Classification and Regression Tree，分类回归树)，从名字就可以看出其不仅可以用于分类，也可以应用于回归。其回归树的建立算法上与分类树部分相似，这里简单介绍下不同之处。</p>
<h5 id="3-6-1-连续值处理"><a href="#3-6-1-连续值处理" class="headerlink" title="3.6.1 连续值处理"></a>3.6.1 连续值处理</h5><p>对于连续值的处理，CART 分类树采用基尼系数的大小来度量特征的各个划分点。在回归模型中，我们使用常见的和方差度量方式，对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集$D_1$和 $D_2$，求出使$D_1$和$D_2$ 各自集合的均方差最小，同时$D_1$和$D_2$ 的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p><img src="/2020/07/25/决策树/1595651148743.png" alt="1595651148743"></p>
<p>其中， $c_1$为 $D_1$ 数据集的样本输出均值， $c_2$为 $D_2$ 数据集的样本输出均值。</p>
<h5 id="3-6-2-预测方式"><a href="#3-6-2-预测方式" class="headerlink" title="3.6.2 预测方式"></a>3.6.2 预测方式</h5><p>对于决策树建立后做预测的方式，上面讲到了 CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。</p>
<ul>
<li><strong>划分标准的差异</strong>：ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于可取值较少的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异</strong>：ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异</strong>：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异</strong>：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；</li>
<li><strong>剪枝策略的差异</strong>：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。</li>
</ul>
<hr>
<p><img src="/2020/07/25/决策树/1595649034648.png" alt="1595649034648"></p>
<p><img src="/2020/07/25/决策树/1595649090585.png" alt="1595649090585"></p>
<p><img src="/2020/07/25/决策树/1595649112772.png" alt="1595649112772"></p>
<p><img src="/2020/07/25/决策树/1595649135709.png" alt="1595649135709"></p>
<p>信息增益，信息增益率 -&gt; 选最大</p>
<p>基尼系数 -&gt; 选最小</p>
<hr>
<h2 id="篇二：Random-Forest-Adaboost-GBDT算法"><a href="#篇二：Random-Forest-Adaboost-GBDT算法" class="headerlink" title="篇二：Random Forest, Adaboost, GBDT算法"></a>篇二：Random Forest, Adaboost, GBDT算法</h2><blockquote>
<p>本部分内容参考公众号Datawhale</p>
<p><a href="https://mp.weixin.qq.com/s/Nl_-PdF0nHBq8yGp6AdI-Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Nl_-PdF0nHBq8yGp6AdI-Q</a></p>
<p>以及博客：</p>
<p><a href="https://blog.csdn.net/sb19931201/article/details/52506157&gt;" target="_blank" rel="noopener">https://blog.csdn.net/sb19931201/article/details/52506157&gt;</a> </p>
</blockquote>
<p>主要介绍基于集成学习的决策树，其主要通过不同学习框架生产基学习器（base learners），并综合所有基学习器的预测结果来改善单个基学习器的识别率和泛化性。</p>
<h3 id="1-集成学习"><a href="#1-集成学习" class="headerlink" title="1.集成学习"></a>1.集成学习</h3><p>常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。</p>
<blockquote>
<ul>
<li><p>In <strong>averaging methods</strong>, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</p>
<p><strong>Examples:</strong> <a href="https://scikit-learn.org/stable/modules/ensemble.html#bagging" target="_blank" rel="noopener">Bagging methods</a>, <a href="https://scikit-learn.org/stable/modules/ensemble.html#forest" target="_blank" rel="noopener">Forests of randomized trees</a>, …</p>
</li>
<li><p>By contrast, in <strong>boosting methods</strong>, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</p>
<p><strong>Examples:</strong> <a href="https://scikit-learn.org/stable/modules/ensemble.html#adaboost" target="_blank" rel="noopener">AdaBoost</a>, <a href="https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting" target="_blank" rel="noopener">Gradient Tree Boosting</a>, …</p>
<p>(来自 <a href="https://scikit-learn.org/stable/modules/ensemble.html" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/ensemble.html</a>)</p>
</li>
</ul>
</blockquote>
<h4 id="1-1-Bagging"><a href="#1-1-Bagging" class="headerlink" title="1.1 Bagging"></a>1.1 Bagging</h4><p>Bagging 全称叫 Bootstrap aggregating，看到 Bootstrap 我们立刻想到著名的开源前端框架（抖个机灵，是 Bootstrap 抽样方法） ，每个基学习器都会对训练集进行<strong>有放回抽样</strong>得到子训练集，比较著名的采样法为 0.632 自助法。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是投票法，票数最多的类别为预测类别。</p>
<p><img src="/2020/07/25/决策树/1595652338160.png" alt="1595652338160"></p>
<h4 id="1-2-Boosting"><a href="#1-2-Boosting" class="headerlink" title="1.2 Boosting"></a>1.2 Boosting</h4><p>Boosting 训练过程为阶梯状，<strong>基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习</strong>，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为加权法。</p>
<p><img src="/2020/07/25/决策树/1595652444294.png" alt="1595652444294"></p>
<h4 id="1-3-Stacking"><a href="#1-3-Stacking" class="headerlink" title="1.3 Stacking"></a>1.3 Stacking</h4><p>Stacking 是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行预测，其预测值将作为训练样本的特征值，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。</p>
<blockquote>
<p>为什么集成学习会好于单个学习器呢？原因可能有三：</p>
<ol>
<li>训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；</li>
<li>假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；</li>
<li>可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。</li>
</ol>
</blockquote>
<h3 id="2-偏差与方差"><a href="#2-偏差与方差" class="headerlink" title="2.偏差与方差"></a>2.偏差与方差</h3><p>如何从偏差和方差的角度来理解集成学习。</p>
<h4 id="2-1-集成学习的偏差与方差"><a href="#2-1-集成学习的偏差与方差" class="headerlink" title="2.1 集成学习的偏差与方差"></a>2.1 集成学习的偏差与方差</h4><p><strong>偏差（Bias）描述的是预测值和真实值之差；方差（Variance）描述的是预测值作为随机变量的离散程度（不同样本下模型效果的离散程度）。</strong></p>
<p>放一张很经典的图：</p>
<p><img src="/2020/07/25/决策树/1595652870653.png" alt="1595652870653"></p>
<blockquote>
<p>We can create a graphical visualization of bias and variance using a bulls-eye diagram. Imagine that the center of the target is a model that perfectly predicts the correct values. As we move away from the bulls-eye, our predictions get worse and worse. Imagine we can repeat our entire model building process to get a number of separate hits on the target. Each hit represents an individual realization of our model, given the chance variability in the training data we gather. Sometimes we will get a good distribution of training data so we predict very well and we are close to the bulls-eye, while sometimes our training data might be full of outliers or non-standard values resulting in poorer predictions. These different realizations result in a scatter of hits on the target. </p>
<p>(来自 <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" target="_blank" rel="noopener">http://scott.fortmann-roe.com/docs/BiasVariance.html</a>)</p>
</blockquote>
<p>模型的偏差与方差</p>
<ul>
<li><strong>偏差</strong>：描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，描得很准，但不一定很稳；</li>
<li><strong>方差</strong>：描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。</li>
</ul>
<p>我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。<strong><span style="color:red">Bagging 和 Stacking 中的基模型为强模型（偏差低方差高），Boosting 中的基模型为弱模型</span></strong>。</p>
<p><img src="/2020/07/25/决策树/1595653661803.png" alt="1595653661803"></p>
<h4 id="2-2-Bagging-的偏差与方差"><a href="#2-2-Bagging-的偏差与方差" class="headerlink" title="2.2 Bagging 的偏差与方差"></a>2.2 Bagging 的偏差与方差</h4><p><img src="/2020/07/25/决策树/1595653718100.png" alt="1595653718100"></p>
<p>（👆 应为方差公式第二项的改变对整体方差的作用很小？）</p>
<p>在此我们知道了为什么 Bagging 中的基模型一定要为强模型，如果 Bagging 使用弱模型则会导致整体模型的偏差提高，而准确度降低。</p>
<p>Random Forest 是经典的基于 Bagging 框架的模型，并在此基础上通过引入特征采样和样本采样来降低基模型间的相关性，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。</p>
<h4 id="2-3-Boosting-的偏差与方差"><a href="#2-3-Boosting-的偏差与方差" class="headerlink" title="2.3 Boosting 的偏差与方差"></a>2.3 Boosting 的偏差与方差</h4><p>对于 Boosting 来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对 Boosting 化简公式为：</p>
<script type="math/tex; mode=display">
E(F)=r\sum_i^mE(f_i)\\
Var(F)=m^2r^2\sigma^2\rho+mr^2\sigma^2(1-\rho)=m^2r^2\sigma^2</script><p>通过观察整体方差的表达式，我们容易发现：</p>
<ul>
<li>若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting框架中的基模型必须为弱模型</li>
<li>因为基模型为弱模型，导致了每个基模型的准确度都不是很高。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此整体模型的准确度提高。但是准确度一定会无限逼近1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集行的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。</li>
</ul>
<p>基于 Boosting 框架的 Gradient Boosting Decision Tree 模型中基模型也为树模型，同 Random Forest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<h4 id="2-4-小结"><a href="#2-4-小结" class="headerlink" title="2.4 小结"></a>2.4 小结</h4><ul>
<li>我们可以使用模型的偏差和方差来近似描述模型的准确度；</li>
<li>对于 Bagging 来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；</li>
<li>对于 Boosting 来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</li>
</ul>
<h3 id="3-Random-Forest"><a href="#3-Random-Forest" class="headerlink" title="3.Random Forest"></a>3.Random Forest</h3><p>RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。<span style="color:red">组成随机森林的树可以是分类树，也可以是回归树。</span>（而GBDT只由回归树组成）</p>
<h4 id="3-1-思想-1"><a href="#3-1-思想-1" class="headerlink" title="3.1 思想"></a>3.1 思想</h4><p>Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：</p>
<ol>
<li>随机选择样本（有放回抽样）；</li>
<li>随机选择特征；</li>
<li>构建决策树；</li>
<li>随机森林投票（平均）。</li>
</ol>
<p>随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；随机选择特征是指在每个节点在分裂过程中都是随机选择特征的（区别于每棵树随机选择一批特征）。</p>
<blockquote>
<p>for each split on a tree, you take a random subset of the features but still use the same splitting criteria. </p>
<p><strong>Does random forest select a subset of features for every tree or every node?</strong></p>
<p>That’s a good question, since the earlier random decision forests by Tin Kam Ho used the “random subspace method,” where each tree got a random subset of features.</p>
<blockquote>
<p>“Our method relies on an autonomous, pseudo-random procedure to select a small number of dimensions from a given feature space …”</p>
</blockquote>
<ul>
<li>Ho, Tin Kam. “The random subspace method for constructing decision forests.” IEEE transactions on pattern analysis and machine intelligence 20.8 (1998): 832-844.</li>
</ul>
<p>However, a few years later, Leo Breiman described the procedure of selecting different subsets of features for each node (while a tree was given the full set of features) — Leo Breiman’s formulation has become the “trademark” random forest algorithm that we typically refer to these days when we speak of “random forest”</p>
<blockquote>
<p>“… random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on.”</p>
</blockquote>
<ul>
<li>Breiman, Leo. “Random Forests” Machine learning 45.1 (2001): 5-32.</li>
</ul>
<p>To answer your question: Each tree gets the full set of features, but at each node, only a random subset of features is considered.</p>
<p>(来自 <a href="https://sebastianraschka.com/faq/docs/random-forest-feature-subsets.html" target="_blank" rel="noopener">https://sebastianraschka.com/faq/docs/random-forest-feature-subsets.html</a>)</p>
</blockquote>
<p>这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。</p>
<p>随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算不剪枝也不会出现过拟合。</p>
<h4 id="3-2-优缺点"><a href="#3-2-优缺点" class="headerlink" title="3.2 优缺点"></a>3.2 优缺点</h4><p>优点</p>
<ol>
<li>在数据集上表现良好，相对于其他算法有较大的优势</li>
<li>易于并行化，在大数据集上有很大的优势；</li>
<li>能够处理高维度数据，不用做特征选择。</li>
</ol>
<h3 id="4-AdaBoost"><a href="#4-AdaBoost" class="headerlink" title="4.AdaBoost"></a>4.AdaBoost</h3><p>AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。</p>
<h4 id="4-1-思想"><a href="#4-1-思想" class="headerlink" title="4.1 思想"></a>4.1 思想</h4><p>Adaboost 迭代算法有三步：</p>
<ol>
<li>初始化训练样本的权值分布，每个样本具有相同权重；</li>
<li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li>
<li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。</li>
</ol>
<h4 id="4-2-细节"><a href="#4-2-细节" class="headerlink" title="4.2 细节"></a>4.2 细节</h4><h5 id="4-2-1-算法过程"><a href="#4-2-1-算法过程" class="headerlink" title="4.2.1 算法过程"></a>4.2.1 算法过程</h5><p>来自 <a href="https://blog.csdn.net/weixin_38629654/article/details/80516045" target="_blank" rel="noopener">https://blog.csdn.net/weixin_38629654/article/details/80516045</a></p>
<p><img src="/2020/07/25/决策树/1601196695960.png" alt="1601196695960"></p>
<p>最后总结一下Adaboost算法，理解它的关键就在于了解权值分布$D_t$是如何影响误差率$e_t$、权值$\alpha_t$以及更新权值分布$D_{t+1}$。具体的例子，可见 <a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">https://blog.csdn.net/guyuealian/article/details/70995333</a></p>
<h5 id="4-2-2-损失函数"><a href="#4-2-2-损失函数" class="headerlink" title="4.2.2 损失函数"></a>4.2.2 损失函数</h5><p><img src="/2020/07/25/决策树/1595663386859.png" alt="1595663386859"></p>
<p><img src="/2020/07/25/决策树/1595663456133.png" alt="1595663456133"></p>
<p><img src="/2020/07/25/决策树/1595663508910.png" alt="1595663508910"></p>
<h5 id="4-2-3-正则化"><a href="#4-2-3-正则化" class="headerlink" title="4.2.3 正则化"></a>4.2.3 正则化</h5><p><img src="/2020/07/25/决策树/1595663617670.png" alt="1595663617670"></p>
<h4 id="4-3-优缺点"><a href="#4-3-优缺点" class="headerlink" title="4.3 优缺点"></a>4.3 优缺点</h4><p><strong>优点</strong></p>
<ol>
<li>分类精度高；</li>
<li>可以用各种回归分类模型来构建弱学习器，非常灵活；</li>
<li>不容易发生过拟合。</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>对异常点敏感，异常点会获得较高权重。</li>
</ol>
<p>另：</p>
<p><strong>优点</strong> </p>
<p>（1）Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也不存在过拟合的现象。 </p>
<p>（2）Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。 </p>
<p>（3）Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。 </p>
<p>（4）Adaboost可以在不改变训练数据，只改变数据权值分布，使得数据在不同学习器中产生不同作用，类似于重采样。 </p>
<p><strong>缺点</strong> </p>
<p>​     在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。</p>
<h3 id="5-GBDT"><a href="#5-GBDT" class="headerlink" title="5.GBDT"></a>5.GBDT</h3><p>GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于 Boosting 策略。GBDT 是被公认的泛化能力较强的算法。</p>
<h4 id="5-1-思想"><a href="#5-1-思想" class="headerlink" title="5.1 思想"></a>5.1 思想</h4><p>GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和Shringkage（一个重要演变）</p>
<h5 id="5-1-1-回归树（Regression-Decision-Tree）"><a href="#5-1-1-回归树（Regression-Decision-Tree）" class="headerlink" title="5.1.1 回归树（Regression Decision Tree）"></a>5.1.1 回归树（Regression Decision Tree）</h5><p>如果认为 GBDT 由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。<strong>GBDT 的核心在于累加所有树的结果作为最终结果</strong>，所以 <strong><span style="color:red">GBDT 中的树都是回归树</span></strong>，不是分类树，这一点相当重要。</p>
<p>回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。</p>
<h5 id="5-1-2-梯度迭代（Gradient-Boosting）"><a href="#5-1-2-梯度迭代（Gradient-Boosting）" class="headerlink" title="5.1.2 梯度迭代（Gradient Boosting）"></a>5.1.2 梯度迭代（Gradient Boosting）</h5><p>上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，<strong>GBDT 的每一棵树都是以之前树得到的残差来更新目标值，这样每一棵树的值加起来即为 GBDT 的预测值</strong>。</p>
<p><img src="/2020/07/25/决策树/1595667008154.png" alt="1595667008154"></p>
<blockquote>
<p>举个例子：比如说 A 用户年龄 20 岁，第一棵树预测 12 岁，那么残差就是 8，第二棵树用 8 来学习，假设其预测为 5，那么其残差即为 3，如此继续学习即可。</p>
</blockquote>
<p><img src="/2020/07/25/决策树/1595667099649.png" alt="1595667099649"></p>
<p><img src="/2020/07/25/决策树/1595667123278.png" alt="1595667123278"></p>
<p><img src="/2020/07/25/决策树/1595667175530.png" alt="1595667175530"></p>
<p>GBDT 的 Boosting 不同于 Adaboost 的 Boosting，GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对于分对样本的权重趋于 0，这样后面的树就能专注于那些被分错的样本。</p>
<hr>
<p>参考 <a href="https://blog.csdn.net/sb19931201/article/details/52506157" target="_blank" rel="noopener">https://blog.csdn.net/sb19931201/article/details/52506157</a></p>
<p>GBM（提升器）算法，又名GBDT，是基于梯度下降算法得到提升树模型。它与提升树的关键不同之处，就在于残差更新的方式。</p>
<p>提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步的优化是简单的。但对于一般的损失函数，每一步的优化并不是那么容易，针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，关键在于利用损失函数的负梯度在当前模型的值 </p>
<script type="math/tex; mode=display">
-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}</script><p>作为回归问题提升树算法中的<strong>残差</strong>的近似值，拟合一个回归树。</p>
<p>GBDT算法步骤：</p>
<p><img src="/2020/07/25/决策树/1601189631302.png" alt="1601189631302"></p>
<p>算法第 1 步初始化 ，估计使损失函数极小化的常数值，它是只有一个根结点的树。第 2 (a) 步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。第 2 (b) 步估计回归树叶结点区域，以拟合残差的近似值。第 2(c) 步利用线性搜索估计叶结点区域的值，使损失函数极小化。第 2(d) 步更新回归树，第 3 步得到输出的最终模型。</p>
<hr>
<p>维基百科 <a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gradient_boosting</a></p>
<p>In pseudocode, the generic gradient boosting method is:</p>
<p><img src="/2020/07/25/决策树/1601190228458.png" alt="1601190228458"></p>
<p><img src="/2020/07/25/决策树/1601190289236.png" alt="1601190289236"></p>
<p><img src="/2020/07/25/决策树/1601190443051.png" alt="1601190443051"></p>
<h5 id="5-1-3-缩减（Shrinkage）"><a href="#5-1-3-缩减（Shrinkage）" class="headerlink" title="5.1.3 缩减（Shrinkage）"></a>5.1.3 缩减（Shrinkage）</h5><p>避免过拟合</p>
<p>Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。</p>
<p>更新规则：</p>
<script type="math/tex; mode=display">
F_m(x)=F_{m-1}(x)+\nu \cdot \gamma_m h_m(x), (0< v \leq 1)</script><p>Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大。</p>
<blockquote>
<p>Empirically it has been found that using small <a href="https://en.wikipedia.org/wiki/Learning_rate" target="_blank" rel="noopener">learning rates</a> (such as  $\nu$ &lt;0.1) yields dramatic improvements in models’ generalization ability over gradient boosting without shrinking ($\nu$ =1). However, it comes at the price of increasing <a href="https://en.wikipedia.org/wiki/Computational_time" target="_blank" rel="noopener">computational time</a> both during training and <a href="https://en.wikipedia.org/wiki/Information_retrieval" target="_blank" rel="noopener">querying</a>: lower learning rate requires more iterations.</p>
<p>（来自 <a href="https://en.wikipedia.org/wiki/Gradient_boosting）" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gradient_boosting）</a></p>
</blockquote>
<h5 id="5-1-4-GBDT总结"><a href="#5-1-4-GBDT总结" class="headerlink" title="5.1.4 GBDT总结"></a>5.1.4 GBDT总结</h5><p>来自 <a href="https://blog.csdn.net/sb19931201/article/details/52506157" target="_blank" rel="noopener">https://blog.csdn.net/sb19931201/article/details/52506157</a> </p>
<ol>
<li><p>Gradient Boosting：每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少（当然也可以变向理解成每个新建的模型都给予上一模型的误差给了更多的关注），与传统Boost对正确、错误的样本进行直接加权还是有区别的。</p>
</li>
<li><p>Size of trees(J)：模型叶结点的数量J需要根据现有数据调整，它控制着模型变量之间的相互作用，一般情况下都选择J的大小在4到8之间，J=2一般不能满足需要，J&gt;10的情况也不太可能需要。</p>
<p><img src="/2020/07/25/决策树/1601191484592.png" alt="1601191484592"></p>
</li>
</ol>
<h4 id="5-2-优缺点"><a href="#5-2-优缺点" class="headerlink" title="5.2 优缺点"></a>5.2 优缺点</h4><p><strong>优点</strong></p>
<ol>
<li>可以自动进行特征组合，拟合非线性数据；</li>
<li>可以灵活处理各种类型的数据。</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>对异常点敏感。</li>
</ol>
<p>另：</p>
<p><img src="/2020/07/25/决策树/1595669502617.png" alt="1595669502617"></p>
<p><img src="/2020/07/25/决策树/1595669541744.png" alt="1595669541744"></p>
<h4 id="5-3-与-Adaboost-的对比"><a href="#5-3-与-Adaboost-的对比" class="headerlink" title="5.3 与 Adaboost 的对比"></a>5.3 与 Adaboost 的对比</h4><p>相同：</p>
<ol>
<li>都是 Boosting 家族成员，使用弱分类器；</li>
<li>都使用前向分布算法；</li>
</ol>
<p>不同：</p>
<ol>
<li>迭代思路不同：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；</li>
<li>损失函数不同：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；</li>
</ol>
<hr>
<h2 id="篇三：XGBoost"><a href="#篇三：XGBoost" class="headerlink" title="篇三：XGBoost"></a>篇三：XGBoost</h2><p><img src="/2020/07/25/决策树/1595667573239.png" alt="1595667573239"></p>
<blockquote>
<p>本部分内容参考公众号Datawhale</p>
<p><a href="https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ</a></p>
<p>以及文章：</p>
<p> <a href="https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A</a></p>
</blockquote>
<p>（注：以下内容只包含XGBoost部分，LightGBM部分可见原文链接）</p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包，比常见的工具包快 10 倍以上。Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是目标函数的定义。</p>
<h4 id="1-1-数学原理"><a href="#1-1-数学原理" class="headerlink" title="1.1 数学原理"></a>1.1 数学原理</h4><h5 id="1-1-1-目标函数"><a href="#1-1-1-目标函数" class="headerlink" title="1.1.1 目标函数"></a>1.1.1 目标函数</h5><p><img src="/2020/07/25/决策树/1595667719081.png" alt="1595667719081"></p>
<p><img src="/2020/07/25/决策树/1595667791401.png" alt="1595667791401"></p>
<p>👆 <span style="color:blue">以基模型为回归树为例，我们的目标其实就是训练一群回归树，使这群树的预测值尽量接近真实值，并且有尽可能强大的泛化能力。前一项表示的是预测误差，后一项表示的是树的复杂度函数，$f_t$表示第t棵树，一共有k棵树。</span></p>
<p>我们要做的就是使<strong>预测误差尽量小，叶子节点数尽量少，预测值尽量不极端</strong>（什么叫预测值尽量不极端?举个栗子，一个人的真实年龄是4岁，有两个模型，第一个模型的第一颗回归树预测值是3岁，第二颗回归树预测值是1岁，第二个模型的第一颗回归树预测值是2岁，第二颗预测值也是2岁，那我们更倾向于选择第二个模型，因为第一个模型学习的太多，有过拟合的风险）</p>
<p><img src="/2020/07/25/决策树/1595667837751.png" alt="1595667837751"></p>
<p><img src="/2020/07/25/决策树/1595667861205.png" alt="1595667861205"></p>
<blockquote>
<script type="math/tex; mode=display">
g_i=\frac{\partial\ l(y_i,\hat{y_i}^{t-1})}{\partial\,\hat{y_i}^{t-1}}</script><script type="math/tex; mode=display">
h_i=\frac{\partial^2 l(y_i,\hat{y_i}^{t-1})}{\partial (\hat{y_i}^{t-1})^2}</script></blockquote>
<p><img src="/2020/07/25/决策树/1595667888425.png" alt="1595667888425"></p>
<p>又因为有 </p>
<script type="math/tex; mode=display">
\sum_{i=1}^{t} \Omega(f_i)=\Omega(f_t)+\sum_{i=1}^{t-1}\Omega(f_i)</script><p>并且$\sum_{i=1}^{t-1}\Omega(f_i)$在前t-1棵树已知下为常数，所以目标函数可写为：</p>
<script type="math/tex; mode=display">
Obj^{(t)}≈\sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)</script><p><span style="color:red">（最优化目标函数，就相当于求解$f_{t}$）</span></p>
<h5 id="1-1-2-基于决策树的目标函数"><a href="#1-1-2-基于决策树的目标函数" class="headerlink" title="1.1.2 基于决策树的目标函数"></a>1.1.2 基于决策树的目标函数</h5><p>我们知道 <span style="color:red">Xgboost 的基模型不仅支持决策树，还支持线性模型</span>，这里我们主要介绍基于决策树的目标函数。</p>
<p><img src="/2020/07/25/决策树/1595667950419.png" alt="1595667950419"></p>
<p>(注：这里的节点权重$w_{j}$指的就是该叶子节点的预测值)</p>
<hr>
<p>另一种描述：</p>
<blockquote>
<p>参考 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
</blockquote>
<p>现在定义$\Omega(f_t)$：假设我们待训练的第t棵树有T个叶子结点，叶子结点的输出向量表示如下：</p>
<script type="math/tex; mode=display">
[w_1,w_2,...,w_T]</script><p>假设 $q(x):R^d \rightarrow \{1,2,3,…,T\}$ 表示样本到叶子结点的映射，那么$f_t(x)=w_{q(x)},w∈R^T$.那么我们定义：</p>
<script type="math/tex; mode=display">
\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2</script><p>其中$T$为叶子结点数，$w_j$为叶子结点$j$的输出，$\gamma$为系数。</p>
<hr>
<p><img src="/2020/07/25/决策树/1595667972342.png" alt="1595667972342"></p>
<p><img src="/2020/07/25/决策树/1595667995025.png" alt="1595667995025"></p>
<p><img src="/2020/07/25/决策树/1595668020156.png" alt="1595668020156"></p>
<hr>
<p>另一种描述：</p>
<blockquote>
<p>参考 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
</blockquote>
<p><img src="/2020/07/25/决策树/1595822346216.png" alt="1595822346216"></p>
<hr>
<p><img src="/2020/07/25/决策树/1595668057942.png" alt="1595668057942"></p>
<h5 id="1-1-3-最优切分点划分算法"><a href="#1-1-3-最优切分点划分算法" class="headerlink" title="1.1.3 最优切分点划分算法"></a>1.1.3 最优切分点划分算法</h5><p>我们刚刚的假设前提是已知前t-1棵树，现在我们讨论怎么生成树。根据决策树的生成策略，在每次分裂节点的时候我们需要考虑能使得损失函数减小最快的节点，也就是分裂后损失函数减去分裂前损失函数我们称之为Gain：</p>
<p><img src="/2020/07/25/决策树/1595822888787.png" alt="1595822888787"></p>
<p>Gain越大越能说明分裂后目标函数值减小越多。</p>
<p>如何找到叶子的节点的最优切分点：Xgboost 支持两种分裂节点的方法——贪心算法和近似算法。</p>
<h6 id="1）贪心算法-精确贪心算法-Basic-Exact-Greedy-Algorithm"><a href="#1）贪心算法-精确贪心算法-Basic-Exact-Greedy-Algorithm" class="headerlink" title="1）贪心算法 (精确贪心算法 Basic Exact Greedy Algorithm)"></a>1）贪心算法 (精确贪心算法 Basic Exact Greedy Algorithm)</h6><p>在决策树（CART）里面，我们使用的是精确贪心算法，也就是将所有特征的所有取值排序（耗时耗内存巨大），然后比较每一个点的Gini，找出变化最大的节点。当特征是连续特征时，我们对连续值离散化，取两点的平均值为分割节点。可以看到，这里的排序算法需要花费大量的时间，因为要遍历整个样本所有特征，而且还要排序。</p>
<p>步骤：</p>
<ol>
<li>从深度为 0 的树开始，对每个叶节点枚举所有的可用特征；</li>
<li>针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；</li>
<li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集</li>
<li>回到第 1 步，递归执行到满足特定条件为止</li>
</ol>
<p>那么如何计算每个特征的分裂收益呢？</p>
<p><img src="/2020/07/25/决策树/1595668137245.png" alt="1595668137245"></p>
<p><img src="/2020/07/25/决策树/1595668154967.png" alt="1595668154967"></p>
<p><img src="/2020/07/25/决策树/1595823332714.png" alt="1595823332714"></p>
<h6 id="2）近似算法-Approximate-Algorithm"><a href="#2）近似算法-Approximate-Algorithm" class="headerlink" title="2）近似算法 (Approximate Algorithm)"></a>2）近似算法 (Approximate Algorithm)</h6><p>贪婪算法可以得到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪婪算法这一缺点给出了近似最优解。</p>
<p>对于每个特征，只考察分位点可以减少计算复杂度。</p>
<p>该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。</p>
<hr>
<p>另一种描述：</p>
<blockquote>
<p>参考 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
</blockquote>
<p>该算法首先根据特征分布的百分位数(percentiles)提出候选分裂点，将连续特征映射到由这些候选点分割的桶中，汇总统计信息并根据汇总的信息在提案中找到最佳解决方案。对于某个特征k，算法首先根据特征分布的分位数找到特征切割点的候选集合$S_k=\{S_{k_1},S_{k_2},…,S_{k_l}\}$ , 然后将特征k的值根据集合$S_k$ 划分到桶(bucket)中，接着对每个桶内的样本统计值G、H进行累加，最后在这些累计的统计量上寻找最佳分裂点。</p>
<hr>
<p>在提出候选切分点时有两种策略：</p>
<ul>
<li>Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；</li>
<li>Local：每次分裂前将重新提出候选切分点。</li>
</ul>
<p>直观上来看，Local 策略需要更多的计算步骤，而 Global 策略因为节点没有划分所以需要更多的候选点。</p>
<p>下图给出不同种分裂策略的 AUC 变换曲线，横坐标为迭代次数，纵坐标为测试集 AUC，eps 为近似算法的精度，其倒数为桶的数量。</p>
<p><img src="/2020/07/25/决策树/1595668243451.png" alt="1595668243451"></p>
<p>我们可以看到 Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有相似的精度。此外我们还发现，在 eps 取值合理的情况下，分位数策略可以获得与贪婪算法相同的精度。</p>
<p><img src="/2020/07/25/决策树/1595668320876.png" alt="1595668320876"></p>
<p><img src="/2020/07/25/决策树/1595668340241.png" alt="1595668340241"></p>
<h5 id="1-1-4-加权分位数缩略图"><a href="#1-1-4-加权分位数缩略图" class="headerlink" title="1.1.4 加权分位数缩略图"></a>1.1.4 加权分位数缩略图</h5><p><img src="/2020/07/25/决策树/1595668403168.png" alt="1595668403168"></p>
<p><img src="/2020/07/25/决策树/1595668446774.png" alt="1595668446774"></p>
<h5 id="1-1-5-稀疏感知算法"><a href="#1-1-5-稀疏感知算法" class="headerlink" title="1.1.5 稀疏感知算法"></a>1.1.5 稀疏感知算法</h5><p>实际应用中，稀疏数据无法避免，产生稀疏数据的原因：（1）数据缺失；（2）统计上的0；（3）特征表示中的one-hot形式。</p>
<p>在决策树的第一篇文章中我们介绍 CART 树在应对数据缺失时的分裂策略，XGBoost 也给出了其解决方案。</p>
<p>XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</p>
<blockquote>
<p>假设样本的第i个特征缺失，无法使用该特征进行样本划分，那我们就把缺失样本默认的分到某个节点，具体分到哪个节点还要根据算法：</p>
<p>算法思想：分别假设缺失属于左节点和右节点，而且只在不缺失的样本上迭代，分别计算样本属于左子树和右子树的增益，选择增益最大的方向作为缺失数据的默认方向。</p>
</blockquote>
<p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而特征值缺失的样本无需遍历只需直接分配到左右节点，故算法所需遍历的样本量减少，下图可以看到稀疏感知算法比 basic 算法速度块了超过 50 倍。</p>
<p><img src="/2020/07/25/决策树/1595668537184.png" alt="1595668537184"></p>
<h5 id="1-1-6-避免过拟合-正则化、shrinkage与采样技术"><a href="#1-1-6-避免过拟合-正则化、shrinkage与采样技术" class="headerlink" title="1.1.6 避免过拟合(正则化、shrinkage与采样技术)"></a>1.1.6 避免过拟合(正则化、shrinkage与采样技术)</h5><h6 id="1-正则化"><a href="#1-正则化" class="headerlink" title="1) 正则化"></a>1) 正则化</h6><p>一说起过拟合，我们的第一反应就是正则化。XGBoost也是这样做的。</p>
<p>我们在loss_function里看到了正则化项（树的复杂度函数），正则化的目的就是防止过拟合。我们再看看这个函数:</p>
<p><img src="data:image/png;base64,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" alt="Q(f) = TT + —Allw112 "></p>
<p>这里出现了γ和λ，这是XGBoost自己定义的，在使用XGBoost时，你可以设定它们的值，显然，γ越大，表示越希望获得结构简单的树，因为要整体最小化的话就要最小化<em>T</em>。λ越大也是越希望获得结构简单的树。</p>
<h6 id="2-Shrinkage"><a href="#2-Shrinkage" class="headerlink" title="2) Shrinkage"></a>2) Shrinkage</h6><p>除了使用正则化，我们还有shrinkage与采样技来避免过拟合的出现。</p>
<p>所谓的shrinkage就是在每次的迭代产生的树中，对每个叶子结点乘以一个缩减权重，主要的目的就是缩减该次迭代产生的树的影响力，留给后边迭代生成的树更多发挥的空间。</p>
<p>举个栗子：比如第一棵树预测值为3.3，label为4.0，第二棵树才学0.7，那后面的树就没啥可学的了，所以给他打个折扣，比如3折，那么第二棵树训练的残差为4.0-3.3*0.3=3.01，这就可以发挥了啦，以此类推，作用的话，就是防止过拟合。</p>
<h6 id="3-采样技术"><a href="#3-采样技术" class="headerlink" title="3) 采样技术"></a>3) 采样技术</h6><p>采样技术有两种，分别是行采样和列采样。</p>
<p>列采样效果比较好的是按层随机的方法：之前提到，每次分裂节点的时候我们都要遍历所有的特征和分割点，来确定最优分割点。如果加入列采样，我们会在同一层的结点分割前先随机选一部分特征，遍历的时候只用遍历这部分特征就行了。</p>
<p>行采样则是采用bagging的思想，每次只抽取部分样本进行训练，不使用全部的样本，可以增加树的多样性。</p>
<h4 id="1-2-工程实现"><a href="#1-2-工程实现" class="headerlink" title="1.2 工程实现"></a>1.2 工程实现</h4><h5 id="1-2-1-块结构设计-支持并行化"><a href="#1-2-1-块结构设计-支持并行化" class="headerlink" title="1.2.1 块结构设计 - 支持并行化"></a>1.2.1 块结构设计 - 支持并行化</h5><p>一直听别人说XGBoost能并行计算，感觉这才是XGBoost最bug的地方，但是直观上并不好理解，明明每次分裂节点都用到了上一次的结果，明明是个串行执行的过程，并行这个小妖精到底在哪？答案就是：选哪个feature进行分裂的时候，就是在枚举，选择最佳分裂点的时候进行。</p>
<p><strong>注意：同层级节点之间可以并行。节点内选择最佳分裂点，候选分裂点计算增益使用并行。</strong></p>
<p>(来自 <a href="https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A</a>)</p>
<hr>
<p>我们知道，决策树的学习最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序。而 XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</p>
<ul>
<li>每一个块结构包括一个或多个已经排序好的特征；</li>
<li>缺失特征值将不进行排序；</li>
<li>每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p><img src="/2020/07/25/决策树/1595668680481.png" alt="1595668680481"></p>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h5 id="1-2-2-缓存访问优化算法"><a href="#1-2-2-缓存访问优化算法" class="headerlink" title="1.2.2 缓存访问优化算法"></a>1.2.2 缓存访问优化算法</h5><p>块结构的设计可以减少节点分裂时的计算量，但特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。</p>
<p>此外适当调整块大小，也可以有助于缓存优化。</p>
<h5 id="1-2-3-“核外”块计算"><a href="#1-2-3-“核外”块计算" class="headerlink" title="1.2.3 “核外”块计算"></a>1.2.3 “核外”块计算</h5><p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li>块压缩：对 Block 进行按列压缩，并在读取时进行解压；</li>
<li>块拆分：将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h4 id="1-3-优缺点"><a href="#1-3-优缺点" class="headerlink" title="1.3 优缺点"></a>1.3 优缺点</h4><h5 id="1-3-1-优点"><a href="#1-3-1-优点" class="headerlink" title="1.3.1 优点"></a>1.3.1 优点</h5><ol>
<li>精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost  对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li>
<li>灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的Logistic回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li>正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li>
<li>Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li>
<li>列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li>
<li>缺失值处理：XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；</li>
<li>可以并行化操作：块结构可以很好的支持并行计算。</li>
</ol>
<h5 id="1-3-2-缺点"><a href="#1-3-2-缺点" class="headerlink" title="1.3.2 缺点"></a>1.3.2 缺点</h5><ol>
<li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li>
<li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</li>
</ol>
<h4 id="1-4-与GBDT的比较"><a href="#1-4-与GBDT的比较" class="headerlink" title="1.4 与GBDT的比较"></a>1.4 与GBDT的比较</h4><p>来自 <a href="https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A</a></p>
<h5 id="1-4-1-共同点"><a href="#1-4-1-共同点" class="headerlink" title="1.4.1 共同点"></a>1.4.1 共同点</h5><ul>
<li><p>二者都是由一堆回归树构成的模型（本次训练的模型基于上次训练的模型）。传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</p>
</li>
<li><p>二都是最小化损失函数的思想，求最优的决策树。</p>
</li>
</ul>
<h5 id="1-4-2-不同点"><a href="#1-4-2-不同点" class="headerlink" title="1.4.2 不同点"></a>1.4.2 不同点</h5><ul>
<li><p>第一眼的感觉XGBoost更好的地方其实就是<strong>正则化+并行化。</strong></p>
</li>
<li><p>二者最小化损失函数的方法不同。GBDT使用的是Gradient Descent方法，优化时只用到了损失函数的一阶导数信息（有人说残差其实就是这里的梯度，不是很理解），拟合上一个模型产生的残差。而XGBoost使用的则是牛顿法，优化时同时用到了一阶导数和二阶导数信息(−<em>gi</em>(1+<em>hi</em>))，其实这个值就是每个叶子节点的预测值（对推导过程有兴趣的朋友请移步参考博文2）。</p>
</li>
<li><p>GBDT分割点的选择通过枚举训练样本集上的特征值来完成，分割点的选择依据则是减少Loss。 XGBoost使用的则是将最小化的目标函数值作为打分函数，提出了一个Gain的公式来划分样本。</p>
</li>
<li><p>XGBoost还支持自定义损失函数，只要能够二阶泰勒展开的函数都可以作为损失函数。</p>
</li>
<li><p>采样方法（Shrinkage）</p>
</li>
<li><p>分裂结点的算法（Weighted Quantile Sketch）</p>
</li>
</ul>
<h3 id="XGBoost动手实践"><a href="#XGBoost动手实践" class="headerlink" title="XGBoost动手实践"></a>XGBoost动手实践</h3><p>本部分内容来源于 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
<h4 id="1-引入基本工具库"><a href="#1-引入基本工具库" class="headerlink" title="1. 引入基本工具库"></a>1. 引入基本工具库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入基本工具库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">"ggplot"</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h4 id="2-XGBoost原生工具库的上手"><a href="#2-XGBoost原生工具库的上手" class="headerlink" title="2. XGBoost原生工具库的上手"></a>2. XGBoost原生工具库的上手</h4><p>更详细的参数设置见后面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb  <span class="comment"># 引入工具库</span></span><br><span class="line"><span class="comment"># read in data</span></span><br><span class="line"><span class="comment"># XGBoost的专属数据格式，但是也可以用dataframe或者ndarray</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.train'</span>)   </span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.test'</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># specify parameters via map</span></span><br><span class="line"><span class="comment"># 设置XGB的参数，使用字典形式传入</span></span><br><span class="line">param = &#123;<span class="string">'max_depth'</span>:<span class="number">2</span>, <span class="string">'eta'</span>:<span class="number">1</span>, <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span> &#125;    </span><br><span class="line"></span><br><span class="line">num_round = <span class="number">2</span>     <span class="comment"># 使用线程数</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)   <span class="comment"># 训练</span></span><br><span class="line"><span class="comment"># make prediction</span></span><br><span class="line">preds = bst.predict(dtest)   <span class="comment"># 预测</span></span><br></pre></td></tr></table></figure>
<h4 id="3-XGBoost的参数设置"><a href="#3-XGBoost的参数设置" class="headerlink" title="3. XGBoost的参数设置"></a>3. XGBoost的参数设置</h4><p>(括号内的名称为sklearn接口对应的参数名字)</p>
<p><span style="background:yellow">更详细的介绍见 <a href="https://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="noopener">官方文档</a>.</span></p>
<p>XGBoost的参数分为三种：</p>
<h5 id="3-1-通用参数"><a href="#3-1-通用参数" class="headerlink" title="3.1. 通用参数"></a>3.1. 通用参数</h5><blockquote>
<ul>
<li>booster: 使用哪个弱学习器训练，默认gbtree，可选gbtree，gblinear或 dart<ul>
<li>booster参数：控制每一步的booster (tree/regression). 因为tree的性能比线性回归好得多，因此我们很少用线性回归. <code>gbtree</code> and <code>dart</code> use tree based models while <code>gblinear</code> uses linear functions.</li>
</ul>
</li>
<li>nthread：用于运行XGBoost的并行线程数，默认为最大可用线程数</li>
<li>verbosity：打印消息的详细程度。默认值为1，有效值为0（静默），1（警告），2（信息），3（调试）。</li>
</ul>
</blockquote>
<p>tree booster：</p>
<blockquote>
<ul>
<li><p>Tree Booster的参数：</p>
</li>
<li><ul>
<li><p>eta（learning_rate）：learning_rate，在更新中使用步长收缩以防止过度拟合，默认= 0.3，范围：[0,1]；典型值一般设置为：0.01-0.2</p>
</li>
<li><p>gamma（min_split_loss）：默认= 0，分裂节点时，损失函数减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，需要平衡。范围：[0，∞]</p>
</li>
<li><p>max_depth：默认= 6，一棵树的最大深度。增加此值将使模型更复杂，并且更可能过度拟合。范围：[0，∞]</p>
</li>
<li><p>min_child_weight：默认值= 1，如果新分裂的节点的样本权重和小于min_child_weight则停止分裂 。这个可以用来减少过拟合，但是也不能太高，会导致欠拟合。范围：[0，∞]</p>
</li>
<li><p>max_delta_step：默认= 0，允许每个叶子输出的最大增量步长。如果将该值设置为0，则表示没有约束。如果将其设置为正值，则可以帮助使更新步骤更加保守。通常不需要此参数，但是当类极度不平衡时，它可能有助于逻辑回归。将其设置为1-10的值可能有助于控制更新。范围：[0，∞]</p>
</li>
<li><p>subsample：默认值= 1，构建每棵树对样本的采样率，如果设置成0.5，XGBoost会随机选择一半的样本作为训练集。范围：（0,1]</p>
</li>
<li><p>sampling_method：默认= uniform，用于对训练实例进行采样的方法。</p>
</li>
<li><ul>
<li>uniform：每个训练实例的选择概率均等。通常将subsample&gt; = 0.5 设置 为良好的效果。</li>
<li>gradient_based：每个训练实例的选择概率与规则化的梯度绝对值成正比，具体来说就是 $\sqrt{g^2+\lambda h^2}$，subsample可以设置为低至0.1，而不会损失模型精度。</li>
</ul>
</li>
<li><p>colsample_bytree：默认= 1，列采样率，也就是特征采样率。范围为（0，1]</p>
</li>
<li><p>lambda（reg_lambda）：默认=1，L2正则化权重项。增加此值将使模型更加保守。</p>
</li>
<li><p>alpha（reg_alpha）：默认= 0，权重的L1正则化项。增加此值将使模型更加保守。</p>
</li>
<li><p>tree_method：默认=auto，XGBoost中使用的树构建算法。可选：<code>auto</code>, <code>exact</code>, <code>approx</code>, <code>hist</code>, <code>gpu_hist</code></p>
</li>
<li><ul>
<li>auto：使用启发式选择最快的方法。</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>对于小型数据集，将使用精确贪婪<code>exact</code>。</li>
<li>对于较大的数据集，将选择近似算法<code>approx</code>。建议尝试<code>hist</code>，<code>gpu_hist</code>，对于大量的数据有更高的性能。<code>gpu_hist</code>支持 external memory 外部存储器。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>exact：精确的贪婪算法。枚举所有拆分的候选点。</li>
<li>approx：使用分位数和梯度直方图的近似贪婪算法。</li>
<li>hist：更快的直方图优化的近似贪婪算法。（LightGBM也是使用直方图算法）</li>
<li>gpu_hist：GPU hist算法的实现。</li>
</ul>
</li>
<li><p>scale_pos_weight: 默认值1，控制正负权重的平衡，这对于不平衡的类别很有用。Kaggle竞赛一般设置 sum(negative instances) / sum(positive instances)，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛。</p>
</li>
<li><p>num_parallel_tree：默认=1，每次迭代期间构造的并行树的数量。此选项用于支持增强型随机森林。</p>
</li>
<li><p>monotone_constraints：可变单调性的约束，在某些情况下，如果有非常强烈的先验信念认为真实的关系具有一定的质量，则可以使用约束条件来提高模型的预测性能。（例如params_constrained[‘monotone_constraints’] = “(1,-1)”，(1,-1)告诉XGBoost对第一个预测变量施加增加的约束，对第二个预测变量施加减小的约束。）</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<p>linear booster：</p>
<blockquote>
<ul>
<li><p>Linear Booster的参数：</p>
</li>
<li><ul>
<li><p>lambda（reg_lambda）：默认= 0，L2正则化权重项。增加此值将使模型更加保守。归一化为训练示例数。(Normalised to number of training examples.)</p>
</li>
<li><p>alpha（reg_alpha）：默认= 0，权重的L1正则化项。增加此值将使模型更加保守。归一化为训练示例数。</p>
</li>
<li><p>updater：默认= shotgun。</p>
</li>
<li><ul>
<li>shotgun：基于shotgun算法的平行坐标下降算法。使用“ hogwild”并行性，因此每次运行都产生不确定的解决方案。</li>
<li>coord_descent：普通坐标下降算法。同样是多线程的，但仍会产生确定性的解决方案。</li>
</ul>
</li>
<li><p>feature_selector：默认= cyclic。特征选择和排序方法</p>
</li>
<li><ul>
<li>cyclic：通过每次循环一个特征来实现的。</li>
<li>shuffle：类似于cyclic，但是在每次更新之前都有随机的特征变换。</li>
<li>random：一个随机(有放回)特征选择器。</li>
<li>greedy：选择梯度最大的特征。（贪婪选择）</li>
<li>thrifty：近似贪婪特征选择（近似于greedy）</li>
</ul>
</li>
<li><p>top_k：默认值为0，要选择的最重要特征数（在greedy和thrifty内）。The number of top features to select in <code>greedy</code> and <code>thrifty</code> feature selector. The value of 0 means using all the features.</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<h5 id="3-2-任务参数"><a href="#3-2-任务参数" class="headerlink" title="3.2. 任务参数"></a>3.2. 任务参数</h5><p>学习目标参数。这个参数用来控制理想的优化目标和每一步结果的度量方法。</p>
<blockquote>
<ul>
<li><p>objective：默认=<code>reg:squarederror</code>，表示最小平方误差。</p>
</li>
<li><ul>
<li><p><code>reg:squarederror</code>：最小平方误差。</p>
</li>
<li><p><code>reg:squaredlogerror</code>：对数平方损失。</p>
</li>
<li><p><code>reg:logistic</code>：逻辑回归</p>
</li>
<li><p><code>reg:pseudohubererror</code>： 使用伪Huber损失进行回归，这是绝对损失的两倍可微选择。</p>
</li>
<li><p><code>binary:logistic</code>：logistic regression for binary classification, output probability.</p>
</li>
<li><p><code>binary:logitraw</code>：logistic regression for binary classification, output score before logistic transformation.</p>
</li>
<li><p><code>binary:hinge</code>：二元分类的铰链损失(hinge loss)。这使预测为0或1，而不是产生概率。（SVM就是铰链损失函数）</p>
</li>
<li><p><code>count:poisson</code>：计数数据的泊松回归，输出泊松分布的平均值。</p>
</li>
<li><p><code>survival:cox</code>：针对正确的（right censored）生存时间数据进行Cox回归（负值被视为正确的生存时间）。</p>
</li>
<li><p><code>survival:aft</code>：用于检查生存时间数据的加速故障时间模型。</p>
</li>
<li><p><code>aft_loss_distribution</code>：Probabilty Density Function used by <code>survival:aft</code> objective and <code>aft-nloglik</code> metric.</p>
</li>
<li><p><code>multi:softmax</code>：设置XGBoost以使用softmax目标进行多类分类，还需要设置num_class（类数）</p>
</li>
<li><p><code>multi:softprob</code>：与softmax相同，但输出向量，可以进一步重整为矩阵。结果包含属于每个类别的每个数据点的预测概率。</p>
</li>
<li><p><code>rank:pairwise</code>：使用LambdaMART进行成对排名，从而使成对损失最小化。</p>
</li>
<li><p><code>rank:ndcg</code>：使用LambdaMART进行列表式排名，使标准化折让累积收益（NDCG）最大化。</p>
</li>
<li><p><code>rank:map</code>：使用LambdaMART进行列表平均排名，使平均平均精度（MAP）最大化。</p>
</li>
<li><p><code>reg:gamma</code>：使用对数链接(log-link)进行伽马回归。输出是伽马分布的平均值。</p>
</li>
<li><p><code>reg:tweedie</code>：使用对数链接进行Tweedie回归。</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>eval_metric：验证数据的评估指标，将根据objective分配默认指标 (rmse for regression, and error for classification, mean average precision for ranking)，用户可以添加多个评估指标 (Python users: remember to pass the metrics in as list of parameters pairs instead of map)</p>
</li>
<li><ul>
<li><p>rmse，均方根误差；</p>
</li>
<li><p>rmsle：均方根对数误差；Default metric of <code>reg:squaredlogerror</code> objective.</p>
</li>
<li><p>mae：平均绝对误差；</p>
</li>
<li><p>mphe：平均伪Huber错误；</p>
</li>
<li><p>logloss：负对数似然；</p>
</li>
<li><p>error：二元分类错误率； It is calculated as <code>#(wrong cases)/#(all cases)</code></p>
</li>
<li><p>merror：多类分类错误率；</p>
</li>
<li><p>mlogloss：多类logloss；</p>
</li>
<li><p>auc：曲线下面积；</p>
</li>
<li><p>aucpr：PR曲线下的面积；</p>
</li>
<li><p>ndcg：归一化累计折扣；</p>
</li>
<li><p>map：平均精度；</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>seed ：随机数种子，[默认= 0]。</li>
</ul>
</blockquote>
<h5 id="3-3-命令行参数"><a href="#3-3-命令行参数" class="headerlink" title="3.3. 命令行参数"></a>3.3. 命令行参数</h5><p>这里不说了，因为很少用命令行控制台版本。(only used in the console version of XGBoost)</p>
<h4 id="4-XGBoost的调参说明"><a href="#4-XGBoost的调参说明" class="headerlink" title="4. XGBoost的调参说明"></a>4. XGBoost的调参说明</h4><p>参数调优的一般步骤：</p>
<ul>
<li>1.确定（较大）学习速率和提升参数调优的初始值</li>
<li>2.max_depth 和 min_child_weight 参数调优</li>
<li>3.gamma参数调优</li>
<li>4.subsample 和 colsample_bytree 参数调优</li>
<li>5.正则化参数alpha调优</li>
<li>6.降低学习速率和使用更多的决策树</li>
</ul>
<h4 id="5-XGBoost详细攻略"><a href="#5-XGBoost详细攻略" class="headerlink" title="5. XGBoost详细攻略"></a>5. XGBoost详细攻略</h4><p>更详细内容见<a href="https://xgboost.readthedocs.io/en/latest/python/python_intro.html" target="_blank" rel="noopener">官方文档</a></p>
<h5 id="1-安装XGBoost"><a href="#1-安装XGBoost" class="headerlink" title="1). 安装XGBoost"></a>1). 安装XGBoost</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip3 install xgboost</span><br><span class="line">或</span><br><span class="line">pip install xgboost</span><br><span class="line"></span><br><span class="line">更新</span><br><span class="line">pip install --upgrade xgboost</span><br></pre></td></tr></table></figure>
<h5 id="2-数据接口（XGBoost可处理的数据格式DMatrix）"><a href="#2-数据接口（XGBoost可处理的数据格式DMatrix）" class="headerlink" title="2). 数据接口（XGBoost可处理的数据格式DMatrix）"></a>2). 数据接口（XGBoost可处理的数据格式DMatrix）</h5><p>The XGBoost python module is able to load data from:</p>
<ul>
<li>LibSVM text format file</li>
<li>Comma-separated values (CSV) file</li>
<li>NumPy 2D array</li>
<li>SciPy 2D sparse array</li>
<li>cuDF DataFrame</li>
<li>Pandas data frame, and</li>
<li>XGBoost binary buffer file.</li>
</ul>
<p>(See <a href="https://xgboost.readthedocs.io/en/latest/tutorials/input_format.html" target="_blank" rel="noopener">Text Input Format of DMatrix</a> for detailed description of text input format.)</p>
<p>The data is stored in a <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix" target="_blank" rel="noopener"><code>DMatrix</code></a> object.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.LibSVM文本格式文件</span></span><br><span class="line"><span class="comment"># To load a libsvm text file or a XGBoost binary file into DMatrix:</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'train.svm.txt'</span>)</span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'test.svm.buffer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.CSV文件(不能含类别文本变量，如果存在文本变量请做特征处理如one-hot)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Categorical features not supported</span></span><br><span class="line"><span class="string">Note that XGBoost does not provide specialization for categorical features; if your data contains categorical features, load it as a NumPy array first and then perform corresponding preprocessing steps like one-hot encoding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Use Pandas to load CSV files with headers</span></span><br><span class="line"><span class="string">Currently, the DMLC data parser cannot parse CSV files with headers. Use Pandas (see below) to read CSV files with headers.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'train.csv?format=csv&amp;label_column=0'</span>)</span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'test.csv?format=csv&amp;label_column=0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.NumPy数组</span></span><br><span class="line">data = np.random.rand(<span class="number">5</span>, <span class="number">10</span>)  <span class="comment"># 5 entities, each contains 10 features</span></span><br><span class="line">label = np.random.randint(<span class="number">2</span>, size=<span class="number">5</span>)  <span class="comment"># binary target</span></span><br><span class="line">dtrain = xgb.DMatrix(data, label=label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.scipy.sparse数组</span></span><br><span class="line">csr = scipy.sparse.csr_matrix((dat, (row, col)))</span><br><span class="line">dtrain = xgb.DMatrix(csr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.pandas数据框dataframe</span></span><br><span class="line">data = pandas.DataFrame(np.arange(<span class="number">12</span>).reshape((<span class="number">4</span>,<span class="number">3</span>)), columns=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</span><br><span class="line">label = pandas.DataFrame(np.random.randint(<span class="number">2</span>, size=<span class="number">4</span>))</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label)</span><br></pre></td></tr></table></figure>
<p>笔者推荐：先保存到XGBoost二进制文件中将使加载速度更快，然后再加载进来</p>
<p>Saving <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix" target="_blank" rel="noopener"><code>DMatrix</code></a> into a XGBoost binary file will make loading faster:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.保存DMatrix到XGBoost二进制文件中</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'train.svm.txt'</span>)</span><br><span class="line">dtrain.save_binary(<span class="string">'train.buffer'</span>)</span><br><span class="line"><span class="comment"># 2. 缺失值可以用DMatrix构造函数中的默认值替换：</span></span><br><span class="line">dtrain = xgb.DMatrix(data, label=label, missing=<span class="number">-999.0</span>)</span><br><span class="line"><span class="comment"># 3.可以在需要时设置权重：</span></span><br><span class="line">w = np.random.rand(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label, missing=<span class="number">-999.0</span>, weight=w)</span><br></pre></td></tr></table></figure>
<h5 id="3-参数的设置方式"><a href="#3-参数的设置方式" class="headerlink" title="3). 参数的设置方式"></a>3). 参数的设置方式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载并处理数据</span></span><br><span class="line">df_wine = pd.read_csv(<span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'</span>,header=<span class="literal">None</span>)</span><br><span class="line">df_wine.columns = [<span class="string">'Class label'</span>, <span class="string">'Alcohol'</span>,<span class="string">'Malic acid'</span>, <span class="string">'Ash'</span>,<span class="string">'Alcalinity of ash'</span>,<span class="string">'Magnesium'</span>, <span class="string">'Total phenols'</span>,</span><br><span class="line">                   <span class="string">'Flavanoids'</span>, <span class="string">'Nonflavanoid phenols'</span>,<span class="string">'Proanthocyanins'</span>,<span class="string">'Color intensity'</span>, <span class="string">'Hue'</span>,<span class="string">'OD280/OD315 of diluted wines'</span>,<span class="string">'Proline'</span>]</span><br><span class="line">df_wine = df_wine[df_wine[<span class="string">'Class label'</span>] != <span class="number">1</span>]  <span class="comment"># drop 1 class</span></span><br><span class="line">y = df_wine[<span class="string">'Class label'</span>].values</span><br><span class="line">X = df_wine[[<span class="string">'Alcohol'</span>,<span class="string">'OD280/OD315 of diluted wines'</span>]].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split  <span class="comment"># 切分训练集与测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder   <span class="comment"># 标签化分类变量</span></span><br><span class="line"></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">y = le.fit_transform(y)</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1</span>,stratify=y)</span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, label=y_train)</span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Booster 参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'booster'</span>: <span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'multi:softmax'</span>,  <span class="comment"># 多分类的问题</span></span><br><span class="line">    <span class="string">'num_class'</span>: <span class="number">10</span>,               <span class="comment"># 类别数，与 multisoftmax 并用</span></span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.1</span>,                  <span class="comment"># 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">12</span>,               <span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">    <span class="string">'lambda'</span>: <span class="number">2</span>,                   <span class="comment"># 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。</span></span><br><span class="line">    <span class="string">'subsample'</span>: <span class="number">0.7</span>,              <span class="comment"># 随机采样训练样本</span></span><br><span class="line">    <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,       <span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">    <span class="string">'min_child_weight'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">1</span>,                   <span class="comment"># 设置成1则没有运行信息输出，最好是设置为0.? slient还是verbosity?</span></span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.007</span>,                  <span class="comment"># 如同学习率</span></span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">'nthread'</span>: <span class="number">4</span>,                  <span class="comment"># cpu 线程数</span></span><br><span class="line">    <span class="string">'eval_metric'</span>:<span class="string">'auc'</span></span><br><span class="line">&#125;</span><br><span class="line">plst = params.items()</span><br><span class="line"><span class="comment"># evallist = [(dtest, 'eval'), (dtrain, 'train')]   # 指定验证集</span></span><br></pre></td></tr></table></figure>
<p>XGBoost can use either a list of pairs or a dictionary to set <a href="https://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="noopener">parameters</a>. For instance:</p>
<ul>
<li><p>Booster parameters</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">param = &#123;<span class="string">'max_depth'</span>: <span class="number">2</span>, <span class="string">'eta'</span>: <span class="number">1</span>, <span class="string">'objective'</span>: <span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">param[<span class="string">'nthread'</span>] = <span class="number">4</span></span><br><span class="line">param[<span class="string">'eval_metric'</span>] = <span class="string">'auc'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>You can also specify multiple eval metrics:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">param[<span class="string">'eval_metric'</span>] = [<span class="string">'auc'</span>, <span class="string">'ams@0'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatively:</span></span><br><span class="line"><span class="comment"># plst = param.items()</span></span><br><span class="line"><span class="comment"># plst += [('eval_metric', 'ams@0')]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Specify validations set to watch performance</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evallist = [(dtest, <span class="string">'eval'</span>), (dtrain, <span class="string">'train'</span>)]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="4-训练"><a href="#4-训练" class="headerlink" title="4). 训练"></a>4). 训练</h5><p>注：貌似现在不需要<code>plst = param.items()</code>，直接传 <code>param</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.训练</span></span><br><span class="line">num_round = <span class="number">10</span></span><br><span class="line">bst = xgb.train(plst, dtrain, num_round)</span><br><span class="line"><span class="comment">#bst = xgb.train(plst, dtrain, num_round, evallist )</span></span><br></pre></td></tr></table></figure>
<h5 id="5-保存模型"><a href="#5-保存模型" class="headerlink" title="5). 保存模型"></a>5). 保存模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.保存模型</span></span><br><span class="line"><span class="comment"># After training, the model can be saved.</span></span><br><span class="line">bst.save_model(<span class="string">'0001.model'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The model and its feature map can also be dumped to a text file.</span></span><br><span class="line"><span class="comment"># dump model</span></span><br><span class="line">bst.dump_model(<span class="string">'dump.raw.txt'</span>)</span><br><span class="line"><span class="comment"># dump model with feature map</span></span><br><span class="line"><span class="comment">#bst.dump_model('dump.raw.txt', 'featmap.txt')</span></span><br></pre></td></tr></table></figure>
<h5 id="6-加载保存的模型"><a href="#6-加载保存的模型" class="headerlink" title="6) . 加载保存的模型"></a>6) . 加载保存的模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.加载保存的模型：</span></span><br><span class="line">bst = xgb.Booster(&#123;<span class="string">'nthread'</span>: <span class="number">4</span>&#125;)  <span class="comment"># init model</span></span><br><span class="line">bst.load_model(<span class="string">'0001.model'</span>)  <span class="comment"># load data</span></span><br></pre></td></tr></table></figure>
<h5 id="7-设置早停机制"><a href="#7-设置早停机制" class="headerlink" title="7). 设置早停机制"></a>7). 设置早停机制</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5.也可以设置早停机制（需要设置验证集）</span></span><br><span class="line">train(..., evals=evals, early_stopping_rounds=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>The model will train until the validation score stops improving. Validation error needs to decrease at least every <code>early_stopping_rounds</code> to continue training.</p>
<p>If early stopping occurs, the model will have three additional fields: <code>bst.best_score</code>, <code>bst.best_iteration</code> and <code>bst.best_ntree_limit</code>. Note that <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train" target="_blank" rel="noopener"><code>xgboost.train()</code></a> will return a model from the last iteration, not the best one.</p>
<p>This works with both metrics to minimize (RMSE, log loss, etc.) and to maximize (MAP, NDCG, AUC). Note that if you specify more than one evaluation metric the last one in <code>param[&#39;eval_metric&#39;]</code> is used for early stopping.</p>
<h5 id="8-预测"><a href="#8-预测" class="headerlink" title="8). 预测"></a>8). 预测</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6.预测</span></span><br><span class="line">ypred = bst.predict(dtest)</span><br></pre></td></tr></table></figure>
<p>If early stopping is enabled during training, you can get predictions from the best iteration with <code>bst.best_ntree_limit</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)</span><br></pre></td></tr></table></figure>
<h5 id="9-绘图"><a href="#9-绘图" class="headerlink" title="9). 绘图"></a>9). 绘图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.绘制重要性</span></span><br><span class="line"><span class="comment"># This function requires matplotlib to be installed.</span></span><br><span class="line">xgb.plot_importance(bst)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.绘制输出树</span></span><br><span class="line"><span class="comment"># This function requires graphviz and matplotlib.</span></span><br><span class="line"><span class="comment">#xgb.plot_tree(bst, num_trees=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.使用xgboost.to_graphviz()将目标树转换为graphviz</span></span><br><span class="line"><span class="comment"># When you use IPython, you can use the xgboost.to_graphviz() function, which converts the target tree to a graphviz instance. The graphviz instance is automatically rendered in IPython.</span></span><br><span class="line"><span class="comment">#xgb.to_graphviz(bst, num_trees=2)</span></span><br></pre></td></tr></table></figure>
<h4 id="6-实战案例"><a href="#6-实战案例" class="headerlink" title="6. 实战案例"></a>6. 实战案例</h4><h5 id="1-分类案例"><a href="#1-分类案例" class="headerlink" title="1). 分类案例"></a>1). 分类案例</h5><p>a. Iris数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score   <span class="comment"># 准确率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载样本数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1234565</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 算法参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'booster'</span>: <span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'multi:softmax'</span>,</span><br><span class="line">    <span class="string">'num_class'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">'lambda'</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">'subsample'</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">'colsample_bytree'</span>: <span class="number">0.75</span>,</span><br><span class="line">    <span class="string">'min_child_weight'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'nthread'</span>: <span class="number">4</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plst = params.items()</span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, y_train) <span class="comment"># 生成数据集格式</span></span><br><span class="line">num_rounds = <span class="number">500</span></span><br><span class="line">model = xgb.train(plst, dtrain, num_rounds) <span class="comment"># xgboost模型训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测</span></span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line">y_pred = model.predict(dtest)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">print(<span class="string">"accuarcy: %.2f%%"</span> % (accuracy*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示重要特征</span></span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuarcy: 96.67%</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/25/决策树/1595830272543.png" alt="1595830272543"></p>
<p>b. Titanic数据集</p>
<p>来自  <a href="https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/qongHAx-X2SWrUxjk8tg0A</a></p>
<p><img src="/2020/07/25/决策树/1601268650680.png" alt="1601268650680"></p>
<p><img src="/2020/07/25/决策树/1601268679265.png" alt="1601268679265"></p>
<h5 id="2-回归案例"><a href="#2-回归案例" class="headerlink" title="2). 回归案例"></a>2). 回归案例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">boston = load_boston()</span><br><span class="line">X,y = boston.data,boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># XGBoost训练过程</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'booster'</span>: <span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'reg:squarederror'</span>,</span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">'lambda'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'subsample'</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">'min_child_weight'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">'nthread'</span>: <span class="number">4</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, y_train)</span><br><span class="line">num_rounds = <span class="number">300</span></span><br><span class="line">plst = params.items()</span><br><span class="line">model = xgb.train(plst, dtrain, num_rounds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测</span></span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line">ans = model.predict(dtest)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示重要特征</span></span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/25/决策树/1595837055748.png" alt="1595837055748"></p>
<h5 id="3-XGBoost调参（结合sklearn网格搜索）"><a href="#3-XGBoost调参（结合sklearn网格搜索）" class="headerlink" title="3). XGBoost调参（结合sklearn网格搜索）"></a><strong>3). XGBoost调参</strong>（结合sklearn网格搜索）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">col = iris.target_names</span><br><span class="line">train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)   <span class="comment"># 分训练集和验证集</span></span><br><span class="line"></span><br><span class="line">parameters = &#123;</span><br><span class="line">              <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>],</span><br><span class="line">              <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.15</span>],</span><br><span class="line">              <span class="string">'n_estimators'</span>: [<span class="number">500</span>, <span class="number">1000</span>, <span class="number">2000</span>, <span class="number">3000</span>, <span class="number">5000</span>],</span><br><span class="line">              <span class="string">'min_child_weight'</span>: [<span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">              <span class="string">'max_delta_step'</span>: [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">              <span class="string">'subsample'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.95</span>],</span><br><span class="line">              <span class="string">'colsample_bytree'</span>: [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>],</span><br><span class="line">              <span class="string">'reg_alpha'</span>: [<span class="number">0</span>, <span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>, <span class="number">1</span>],</span><br><span class="line">              <span class="string">'reg_lambda'</span>: [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>],</span><br><span class="line">              <span class="string">'scale_pos_weight'</span>: [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">xlf = xgb.XGBClassifier(max_depth=<span class="number">10</span>,</span><br><span class="line">            learning_rate=<span class="number">0.01</span>,</span><br><span class="line">            n_estimators=<span class="number">2000</span>,</span><br><span class="line">            silent=<span class="literal">True</span>,</span><br><span class="line">            objective=<span class="string">'multi:softmax'</span>,</span><br><span class="line">            num_class=<span class="number">3</span> ,</span><br><span class="line">            nthread=<span class="number">-1</span>,</span><br><span class="line">            gamma=<span class="number">0</span>,</span><br><span class="line">            min_child_weight=<span class="number">1</span>,</span><br><span class="line">            max_delta_step=<span class="number">0</span>,</span><br><span class="line">            subsample=<span class="number">0.85</span>,</span><br><span class="line">            colsample_bytree=<span class="number">0.7</span>,</span><br><span class="line">            colsample_bylevel=<span class="number">1</span>,</span><br><span class="line">            reg_alpha=<span class="number">0</span>,</span><br><span class="line">            reg_lambda=<span class="number">1</span>,</span><br><span class="line">            scale_pos_weight=<span class="number">1</span>,</span><br><span class="line">            seed=<span class="number">0</span>,</span><br><span class="line">            missing=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">gs = GridSearchCV(xlf, param_grid=parameters, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">3</span>)</span><br><span class="line">gs.fit(train_x, train_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best score: %0.3f"</span> % gs.best_score_)</span><br><span class="line">print(<span class="string">"Best parameters set: %s"</span> % gs.best_params_ )</span><br></pre></td></tr></table></figure>
<h2 id="决策树算法十问及经典面试问题"><a href="#决策树算法十问及经典面试问题" class="headerlink" title="决策树算法十问及经典面试问题"></a>决策树算法十问及经典面试问题</h2><p>（见 <a href="https://mp.weixin.qq.com/s/vkbZweJ5oRo4IPt-3kg64g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/vkbZweJ5oRo4IPt-3kg64g</a> ）</p>
<h3 id="简介和算法"><a href="#简介和算法" class="headerlink" title="简介和算法"></a>简介和算法</h3><blockquote>
<p>决策树是机器学习最常用的算法之一，它将算法组织成一颗树的形式。其实这就是将平时所说的if-then语句构建成了树的形式。这个决策树主要包括三个部分：内部节点、叶节点和边。内部节点是划分的属性，边代表划分的条件，叶节点表示类别。构建决策树 就是一个递归的选择内部节点，计算划分条件的边，最后到达叶子节点的过程。</p>
</blockquote>
<p><img src="/2020/07/25/决策树/1595669193083.png" alt="1595669193083"></p>
<h3 id="核心公式"><a href="#核心公式" class="headerlink" title="核心公式"></a>核心公式</h3><p><img src="/2020/07/25/决策树/1595669223727.png" alt="1595669223727"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">model</th>
<th style="text-align:center">feature select</th>
<th style="text-align:center">树的类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ID3</td>
<td style="text-align:center">{分类:信息增益}</td>
<td style="text-align:center">多叉树</td>
</tr>
<tr>
<td style="text-align:center">C4.5</td>
<td style="text-align:center">{分类:信息增益率}</td>
<td style="text-align:center">多叉树</td>
</tr>
<tr>
<td style="text-align:center">CART (分类树)</td>
<td style="text-align:center">{分类:基尼指数}</td>
<td style="text-align:center">二叉树</td>
</tr>
<tr>
<td style="text-align:center">CART (回归树)</td>
<td style="text-align:center">{回归:平方误差}</td>
<td style="text-align:center">二叉树</td>
</tr>
</tbody>
</table>
</div>
<h3 id="算法十问"><a href="#算法十问" class="headerlink" title="算法十问"></a>算法十问</h3><p>1.决策树和条件概率分布的关系？</p>
<blockquote>
<p>决策树可以表示成给定条件下类的条件概率分布. 决策树中的每一条路径都对应是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大.</p>
</blockquote>
<p>2.ID3和C4.5算法可以处理实数特征吗？如果可以应该怎么处理？如果不可以请给出理由？</p>
<p><img src="/2020/07/25/决策树/1595669343135.png" alt="1595669343135"></p>
<p>3.既然信息增益可以计算，为什么C4.5还使用信息增益比？</p>
<blockquote>
<p>在使用信息增益的时候，如果某个特征有很多取值，使用这个取值多的特征会的大的信息增益，这个问题是出现很多分支，将数据划分更细，模型复杂度高，出现过拟合的机率更大。使用信息增益比就是为了解决偏向于选择取值较多的特征的问题. 使用信息增益比对取值多的特征加上的惩罚，对这个问题进行了校正.</p>
</blockquote>
<p>4.基尼指数可以表示数据不确定性，信息熵也可以表示数据的不确定性. 为什么CART使用基尼指数？</p>
<blockquote>
<p>信息熵0, logK都是值越大，数据的不确定性越大. 信息熵需要计算对数，计算量大；信息熵是可以处理多个类别，基尼指数就是针对两个类计算的，由于CART树是一个二叉树，每次都是选择yes or no进行划分，从这个角度也是应该选择简单的基尼指数进行计算.</p>
</blockquote>
<p>5.决策树怎么剪枝？</p>
<blockquote>
<p>一般算法在构造决策树的都是尽可能的细分，直到数据不可划分才会到达叶子节点，停止划分. 因为给训练数据巨大的信任，这种形式形式很容易造成过拟合，为了防止过拟合需要进行决策树剪枝. 一般分为预剪枝和后剪枝，预剪枝是在决策树的构建过程中加入限制，比如控制叶子节点最少的样本个数，提前停止. 后剪枝是在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒.</p>
</blockquote>
<p>6.ID3算法，为什么不选择具有最高预测精度的属性特征，而是使用信息增益？</p>
<p>7.为什么使用贪心和其发生搜索建立决策树，为什么不直接使用暴力搜索建立最优的决策树？</p>
<blockquote>
<p>决策树目的是构建一个与训练数据拟合很好，并且复杂度小的决策树. 因为从所有可能的决策树中直接选择最优的决策树是NP完全问题，在使用中一般使用启发式方法学习相对最优的决策树.</p>
</blockquote>
<p>8.如果特征很多，决策树中最后没有用到的特征一定是无用吗？</p>
<blockquote>
<p>不是无用的，从两个角度考虑，一是特征替代性，如果可以已经使用的特征A和特征B可以提点特征C，特征C可能就没有被使用，但是如果把特征C单独拿出来进行训练，依然有效. 其二，决策树的每一条路径就是计算条件概率的条件，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据。</p>
</blockquote>
<p>9.决策树的优点？</p>
<blockquote>
<p>优点: 决策树模型可读性好，具有描述性，有助于人工分析；效率高，决策树只需要一次性构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。缺点: 对中间值的缺失敏感；可能产生过度匹配的问题，即过拟合。</p>
</blockquote>
<p>10.基尼系数存在的问题?</p>
<blockquote>
<p>基尼指数偏向于多值属性;当类数较大时，基尼指数求解比较困难;基尼指数倾向于支持在两个分区中生成大小相同的测试。</p>
</blockquote>
<h3 id="面试真题"><a href="#面试真题" class="headerlink" title="面试真题"></a>面试真题</h3><ol>
<li>决策树如何防止过拟合？</li>
<li>信息增益比相对信息增益有什么好处？</li>
<li>如果由异常值或者数据分布不均匀，会对决策树有什么影响？</li>
<li>手动构建CART的回归树的前两个节点，给出公式每一步的公式推到？</li>
<li>决策树和其他模型相比有什么优点？</li>
<li>决策树的目标函数是什么？</li>
</ol>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/04/02/Time-Series/" rel="next" title="Time Series">
                <i class="fa fa-chevron-left"></i> Time Series
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/07/28/深度学习相关资料/" rel="prev" title="深度学习相关资料">
                深度学习相关资料 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="qypx">
            
              <p class="site-author-name" itemprop="name">qypx</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">102</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">46</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qypx" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          


          
          

          
          

          


          <!-- 新增的内容 -->
          <!-- require APlayer -->
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
          <script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script>
          <!-- require MetingJS -->
          <script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

          <meting-js server="netease" type="playlist" id="4870130923" list-folded="true" order="random">
          </meting-js>
          <!-- 新增的内容end -->

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#篇零：决策树的分类"><span class="nav-text">篇零：决策树的分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#篇一：决策树-（ID3-C4-5-CART）"><span class="nav-text">篇一：决策树 （ID3, C4.5, CART）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3"><span class="nav-text">ID3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-思想"><span class="nav-text">1.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-划分标准"><span class="nav-text">1.2 划分标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-缺点"><span class="nav-text">1.3 缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C4-5"><span class="nav-text">C4.5</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-思想"><span class="nav-text">2.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-划分标准"><span class="nav-text">2.2 划分标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-剪枝策略"><span class="nav-text">2.3 剪枝策略</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-1-预剪枝"><span class="nav-text">2.3.1 预剪枝</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-2-后剪枝"><span class="nav-text">2.3.2 后剪枝</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-缺点"><span class="nav-text">2.4 缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART"><span class="nav-text">CART</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-思想"><span class="nav-text">3.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-划分标准"><span class="nav-text">3.2 划分标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-缺失值处理"><span class="nav-text">3.3 缺失值处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-剪枝策略"><span class="nav-text">3.4 剪枝策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-类别不平衡"><span class="nav-text">3.5 类别不平衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-回归树"><span class="nav-text">3.6 回归树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-6-1-连续值处理"><span class="nav-text">3.6.1 连续值处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-6-2-预测方式"><span class="nav-text">3.6.2 预测方式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#篇二：Random-Forest-Adaboost-GBDT算法"><span class="nav-text">篇二：Random Forest, Adaboost, GBDT算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-集成学习"><span class="nav-text">1.集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-Bagging"><span class="nav-text">1.1 Bagging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-Boosting"><span class="nav-text">1.2 Boosting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-Stacking"><span class="nav-text">1.3 Stacking</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-偏差与方差"><span class="nav-text">2.偏差与方差</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-集成学习的偏差与方差"><span class="nav-text">2.1 集成学习的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Bagging-的偏差与方差"><span class="nav-text">2.2 Bagging 的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Boosting-的偏差与方差"><span class="nav-text">2.3 Boosting 的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-小结"><span class="nav-text">2.4 小结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Random-Forest"><span class="nav-text">3.Random Forest</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-思想-1"><span class="nav-text">3.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-优缺点"><span class="nav-text">3.2 优缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-AdaBoost"><span class="nav-text">4.AdaBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-思想"><span class="nav-text">4.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-细节"><span class="nav-text">4.2 细节</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-1-算法过程"><span class="nav-text">4.2.1 算法过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-损失函数"><span class="nav-text">4.2.2 损失函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-3-正则化"><span class="nav-text">4.2.3 正则化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-优缺点"><span class="nav-text">4.3 优缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-GBDT"><span class="nav-text">5.GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-思想"><span class="nav-text">5.1 思想</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-1-回归树（Regression-Decision-Tree）"><span class="nav-text">5.1.1 回归树（Regression Decision Tree）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-2-梯度迭代（Gradient-Boosting）"><span class="nav-text">5.1.2 梯度迭代（Gradient Boosting）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-3-缩减（Shrinkage）"><span class="nav-text">5.1.3 缩减（Shrinkage）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-4-GBDT总结"><span class="nav-text">5.1.4 GBDT总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-优缺点"><span class="nav-text">5.2 优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-与-Adaboost-的对比"><span class="nav-text">5.3 与 Adaboost 的对比</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#篇三：XGBoost"><span class="nav-text">篇三：XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost"><span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-数学原理"><span class="nav-text">1.1 数学原理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-1-目标函数"><span class="nav-text">1.1.1 目标函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-2-基于决策树的目标函数"><span class="nav-text">1.1.2 基于决策树的目标函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-3-最优切分点划分算法"><span class="nav-text">1.1.3 最优切分点划分算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1）贪心算法-精确贪心算法-Basic-Exact-Greedy-Algorithm"><span class="nav-text">1）贪心算法 (精确贪心算法 Basic Exact Greedy Algorithm)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2）近似算法-Approximate-Algorithm"><span class="nav-text">2）近似算法 (Approximate Algorithm)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-4-加权分位数缩略图"><span class="nav-text">1.1.4 加权分位数缩略图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-5-稀疏感知算法"><span class="nav-text">1.1.5 稀疏感知算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-6-避免过拟合-正则化、shrinkage与采样技术"><span class="nav-text">1.1.6 避免过拟合(正则化、shrinkage与采样技术)</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-正则化"><span class="nav-text">1) 正则化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-Shrinkage"><span class="nav-text">2) Shrinkage</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-采样技术"><span class="nav-text">3) 采样技术</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-工程实现"><span class="nav-text">1.2 工程实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-1-块结构设计-支持并行化"><span class="nav-text">1.2.1 块结构设计 - 支持并行化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-2-缓存访问优化算法"><span class="nav-text">1.2.2 缓存访问优化算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-3-“核外”块计算"><span class="nav-text">1.2.3 “核外”块计算</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-优缺点"><span class="nav-text">1.3 优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-1-优点"><span class="nav-text">1.3.1 优点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-2-缺点"><span class="nav-text">1.3.2 缺点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-与GBDT的比较"><span class="nav-text">1.4 与GBDT的比较</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-1-共同点"><span class="nav-text">1.4.1 共同点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-2-不同点"><span class="nav-text">1.4.2 不同点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost动手实践"><span class="nav-text">XGBoost动手实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-引入基本工具库"><span class="nav-text">1. 引入基本工具库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-XGBoost原生工具库的上手"><span class="nav-text">2. XGBoost原生工具库的上手</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-XGBoost的参数设置"><span class="nav-text">3. XGBoost的参数设置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-通用参数"><span class="nav-text">3.1. 通用参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-任务参数"><span class="nav-text">3.2. 任务参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-命令行参数"><span class="nav-text">3.3. 命令行参数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-XGBoost的调参说明"><span class="nav-text">4. XGBoost的调参说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-XGBoost详细攻略"><span class="nav-text">5. XGBoost详细攻略</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-安装XGBoost"><span class="nav-text">1). 安装XGBoost</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-数据接口（XGBoost可处理的数据格式DMatrix）"><span class="nav-text">2). 数据接口（XGBoost可处理的数据格式DMatrix）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-参数的设置方式"><span class="nav-text">3). 参数的设置方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-训练"><span class="nav-text">4). 训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-保存模型"><span class="nav-text">5). 保存模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-加载保存的模型"><span class="nav-text">6) . 加载保存的模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-设置早停机制"><span class="nav-text">7). 设置早停机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-预测"><span class="nav-text">8). 预测</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#9-绘图"><span class="nav-text">9). 绘图</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-实战案例"><span class="nav-text">6. 实战案例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-分类案例"><span class="nav-text">1). 分类案例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-回归案例"><span class="nav-text">2). 回归案例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-XGBoost调参（结合sklearn网格搜索）"><span class="nav-text">3). XGBoost调参（结合sklearn网格搜索）</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树算法十问及经典面试问题"><span class="nav-text">决策树算法十问及经典面试问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介和算法"><span class="nav-text">简介和算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核心公式"><span class="nav-text">核心公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法十问"><span class="nav-text">算法十问</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面试真题"><span class="nav-text">面试真题</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qypx</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left"},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
