<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,">










<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="http://qypx.github.io/2020/07/25/决策树/index.html">
<meta property="og:site_name" content="qypx の blog">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649034648.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649090585.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649112772.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649135709.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595649657475.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650023450.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650557293.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650609697.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650752017.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650778439.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595650963697.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595651148743.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595652338160.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595652444294.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595652870653.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595653661803.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595653718100.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595653801559.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663386859.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663456133.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663508910.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595663617670.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667008154.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667099649.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667123278.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667175530.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667249030.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669502617.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669541744.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667573239.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667719081.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667791401.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667837751.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667861205.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667888425.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667950419.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667972342.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595667995025.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668020156.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595822346216.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668057942.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595822888787.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668137245.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668154967.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595823332714.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668243451.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668320876.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668340241.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668403168.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668446774.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668537184.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595668680481.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595830272543.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595837055748.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669193083.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669223727.png">
<meta property="og:image" content="http://qypx.github.io/2020/07/25/决策树/1595669343135.png">
<meta property="og:updated_time" content="2020-07-27T09:02:24.899Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树">
<meta name="twitter:image" content="http://qypx.github.io/2020/07/25/决策树/1595649034648.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://qypx.github.io/2020/07/25/决策树/">





<!-- 网页加载条 -->
<script src="https://neveryu.github.io/js/src/pace.min.js"></script>

  <title>决策树 | qypx の blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">qypx の blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">机会是留给有准备的人的.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://qypx.github.io/2020/07/25/决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qypx">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="qypx の blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-25T13:48:42+10:00">
                2020-07-25
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2020-07-27T19:02:24+10:00">
                2020-07-27
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读量
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>（注：更多关于Boosting系列算法可见OneNote）</p>
<p><img src="/2020/07/25/决策树/1595649034648.png" alt="1595649034648"></p>
<p><img src="/2020/07/25/决策树/1595649090585.png" alt="1595649090585"></p>
<p><img src="/2020/07/25/决策树/1595649112772.png" alt="1595649112772"></p>
<p><img src="/2020/07/25/决策树/1595649135709.png" alt="1595649135709"></p>
<p>信息增益，信息增益率 -&gt; 选最大</p>
<p>基尼系数 -&gt; 选最小</p>
<hr>
<blockquote>
<p>以下内容来源于公众号Datawhale</p>
<p><a href="https://mp.weixin.qq.com/s/jj3BtmnWRAwCS56ZU3ZXZA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/jj3BtmnWRAwCS56ZU3ZXZA</a></p>
</blockquote>
<h2 id="篇一：决策树-（ID3-C4-5-CART）"><a href="#篇一：决策树-（ID3-C4-5-CART）" class="headerlink" title="篇一：决策树 （ID3, C4.5, CART）"></a>篇一：决策树 （ID3, C4.5, CART）</h2><h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><p>ID3 算法是建立在奥卡姆剃刀（用较少的东西，同样可以做好事情）的基础上：越是小型的决策树越优于大的决策树。</p>
<h4 id="1-1-思想"><a href="#1-1-思想" class="headerlink" title="1.1 思想"></a>1.1 思想</h4><p>从信息论的知识中我们知道：期望信息越小，信息熵越大，从而样本纯度越低。ID3 算法的核心思想就是以信息增益来度量特征选择，<strong>选择<span style="color:red">信息增益</span>最大的特征进行分裂</strong>。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。</p>
<p>其大致步骤为：</p>
<ol>
<li>初始化特征集合和数据集合；</li>
<li>计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；</li>
<li>更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；</li>
<li>重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。</li>
</ol>
<h4 id="1-2-划分标准"><a href="#1-2-划分标准" class="headerlink" title="1.2 划分标准"></a>1.2 划分标准</h4><p>ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度。</p>
<p><img src="/2020/07/25/决策树/1595649657475.png" alt="1595649657475"></p>
<p><strong>信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”。</strong></p>
<h4 id="1-3-缺点"><a href="#1-3-缺点" class="headerlink" title="1.3 缺点"></a>1.3 缺点</h4><ul>
<li>ID3 没有剪枝策略，容易过拟合；</li>
<li>信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1（但实际上这并不是一个好的特征选择）；</li>
<li>只能用于处理离散分布的特征；</li>
<li>没有考虑缺失值。</li>
</ul>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p><strong>C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点</strong>，引入<strong><span style="color:red">信息增益率</span></strong>来作为分类标准。</p>
<h4 id="2-1-思想"><a href="#2-1-思想" class="headerlink" title="2.1 思想"></a>2.1 思想</h4><p>C4.5 相对于 ID3 的缺点对应有以下改进方式：</p>
<ul>
<li><p>引入悲观剪枝策略进行后剪枝；</p>
</li>
<li><p>引入信息增益率作为划分标准；</p>
</li>
<li><p>将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；</p>
</li>
<li><p>对于缺失值的处理可以分为两个子问题：1. 在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）2. 选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）</p>
</li>
<li><ul>
<li>针对问题一，C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；</li>
<li>针对问题二，C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</li>
</ul>
</li>
</ul>
<h4 id="2-2-划分标准"><a href="#2-2-划分标准" class="headerlink" title="2.2 划分标准"></a>2.2 划分标准</h4><p><img src="/2020/07/25/决策树/1595650023450.png" alt="1595650023450"></p>
<p>这里需要注意，信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个<em>启发式方法</em>：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p>
<h4 id="2-3-剪枝策略"><a href="#2-3-剪枝策略" class="headerlink" title="2.3 剪枝策略"></a>2.3 剪枝策略</h4><p>为什么要剪枝：过拟合的树在泛化能力的表现非常差。</p>
<h5 id="2-3-1-预剪枝"><a href="#2-3-1-预剪枝" class="headerlink" title="2.3.1 预剪枝"></a>2.3.1 预剪枝</h5><p>在节点划分前来确定是否继续增长，及早停止增长的主要方法有：</p>
<ul>
<li>节点内数据样本低于某一阈值；</li>
<li>所有节点特征都已分裂；</li>
<li>节点划分前准确率比划分后准确率高。</li>
</ul>
<p>预剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。</p>
<h5 id="2-3-2-后剪枝"><a href="#2-3-2-后剪枝" class="headerlink" title="2.3.2 后剪枝"></a>2.3.2 后剪枝</h5><p>在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。</p>
<p><strong>C4.5 采用的悲观剪枝方法</strong>，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。</p>
<p>后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。</p>
<h4 id="2-4-缺点"><a href="#2-4-缺点" class="headerlink" title="2.4 缺点"></a>2.4 缺点</h4><ul>
<li>剪枝策略可以再优化；</li>
<li>C4.5 用的是多叉树，用二叉树效率更高；</li>
<li>C4.5 只能用于分类；</li>
<li>C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；</li>
<li>C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li>
</ul>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>ID3 和 C4.5 虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但是其生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。</p>
<h4 id="3-1-思想"><a href="#3-1-思想" class="headerlink" title="3.1 思想"></a>3.1 思想</h4><p>CART 包含的基本过程有分裂，剪枝和树选择。</p>
<ul>
<li><strong>分裂</strong>：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；</li>
<li><strong>剪枝</strong>：<strong>采用<em>代价复杂度剪枝</em></strong>，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；</li>
<li><strong>树选择</strong>：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。</li>
</ul>
<p>CART 在 C4.5 的基础上进行了很多提升。</p>
<ul>
<li>C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快；</li>
<li>C4.5 只能分类，CART 既可以分类也可以回归；</li>
<li>CART 使用 <strong><span style="color:red">Gini 系数</span></strong>作为变量的不纯度量，减少了大量的对数运算；</li>
<li>CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；</li>
<li>CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。</li>
</ul>
<h4 id="3-2-划分标准"><a href="#3-2-划分标准" class="headerlink" title="3.2 划分标准"></a>3.2 划分标准</h4><p>熵模型拥有大量耗时的对数运算，基尼指数在简化模型的同时还保留了熵模型的优点。<strong>基尼指数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。</strong></p>
<p><img src="/2020/07/25/决策树/1595650557293.png" alt="1595650557293"></p>
<p><img src="/2020/07/25/决策树/1595650609697.png" alt="1595650609697"></p>
<h4 id="3-3-缺失值处理"><a href="#3-3-缺失值处理" class="headerlink" title="3.3 缺失值处理"></a>3.3 缺失值处理</h4><p>上文说到，模型对于缺失值的处理会分为两个子问题：1. 在特征值缺失的情况下进行划分特征的选择？2. 选定该划分特征，对于缺失该特征值的样本如何处理？</p>
<p>对于问题 1，CART 一开始严格要求分裂特征评估时只能使用在该特征上没有缺失值的那部分数据，在后续版本中，CART 算法使用了一种惩罚机制来抑制提升值，从而反映出缺失值的影响（例如，如果一个特征在节点的 20% 的记录是缺失的，那么这个特征就会减少 20% 或者其他数值）。</p>
<p>对于问题 2，CART 算法的机制是为树的每个节点都找到代理分裂器，无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理（即代理就是代替缺失值特征作为划分特征的特征），当 CART 树中遇到缺失值时，这个实例划分到左边还是右边是决定于其排名最高的代理，如果这个代理的值也缺失了，那么就使用排名第二的代理，以此类推，如果所有代理值都缺失，那么默认规则就是把样本划分到较大的那个子节点。代理分裂器可以确保无缺失训练数据上得到的树可以用来处理包含确实值的新数据。</p>
<h4 id="3-4-剪枝策略"><a href="#3-4-剪枝策略" class="headerlink" title="3.4 剪枝策略"></a>3.4 剪枝策略</h4><p>采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这一系列树中的最后一棵树仅含一个用来预测类别的叶节点。然后用一种成本复杂度的度量准则来判断哪棵子树应该被一个预测类别值的叶节点所代替。这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。</p>
<p>我们来看具体看一下代价复杂度剪枝算法：</p>
<p><img src="/2020/07/25/决策树/1595650752017.png" alt="1595650752017"></p>
<p><img src="/2020/07/25/决策树/1595650778439.png" alt="1595650778439"></p>
<h4 id="3-5-类别不平衡"><a href="#3-5-类别不平衡" class="headerlink" title="3.5 类别不平衡"></a>3.5 类别不平衡</h4><p>CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其自动消除不需要建模人员采取其他操作。</p>
<p>CART 使用了一种先验机制，其作用相当于对类别进行加权。这种先验机制嵌入于 CART 算法判断分裂优劣的运算里，在 CART 默认的分类模式中，总是要计算每个节点关于根节点的类别频率的比值，这就相当于对数据自动重加权，对类别进行均衡。</p>
<p><img src="/2020/07/25/决策树/1595650963697.png" alt="1595650963697"></p>
<p>通过这种计算方式就无需管理数据真实的类别分布。假设有 K 个目标类别，就可以确保根节点中每个类别的概率都是 1/K。这种默认的模式被称为“先验相等”。</p>
<p>先验设置和加权不同之处在于先验不影响每个节点中的各类别样本的数量或者份额。先验影响的是每个节点的类别赋值和树生长过程中分裂的选择。</p>
<h4 id="3-6-回归树"><a href="#3-6-回归树" class="headerlink" title="3.6 回归树"></a>3.6 回归树</h4><p>CART(Classification and Regression Tree，分类回归树)，从名字就可以看出其不仅可以用于分类，也可以应用于回归。其回归树的建立算法上与分类树部分相似，这里简单介绍下不同之处。</p>
<h5 id="3-6-1-连续值处理"><a href="#3-6-1-连续值处理" class="headerlink" title="3.6.1 连续值处理"></a>3.6.1 连续值处理</h5><p>对于连续值的处理，CART 分类树采用基尼系数的大小来度量特征的各个划分点。在回归模型中，我们使用常见的和方差度量方式，对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集$D_1$和 $D_2$，求出使$D_1$和$D_2$ 各自集合的均方差最小，同时$D_1$和$D_2$ 的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p><img src="/2020/07/25/决策树/1595651148743.png" alt="1595651148743"></p>
<p>其中， $c_1$为 $D_1$ 数据集的样本输出均值， $c_2$为 $D_2$ 数据集的样本输出均值。</p>
<h5 id="3-6-2-预测方式"><a href="#3-6-2-预测方式" class="headerlink" title="3.6.2 预测方式"></a>3.6.2 预测方式</h5><p>对于决策树建立后做预测的方式，上面讲到了 CART 分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后通过总结的方式对比下 ID3、C4.5 和 CART 三者之间的差异。</p>
<ul>
<li><strong>划分标准的差异</strong>：ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于可取值较少的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。</li>
<li><strong>使用场景的差异</strong>：ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；</li>
<li><strong>样本数据的差异</strong>：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；</li>
<li><strong>样本特征的差异</strong>：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；</li>
<li><strong>剪枝策略的差异</strong>：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。</li>
</ul>
<hr>
<blockquote>
<p>以下内容来源于公众号Datawhale</p>
<p><a href="https://mp.weixin.qq.com/s/Nl_-PdF0nHBq8yGp6AdI-Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Nl_-PdF0nHBq8yGp6AdI-Q</a></p>
</blockquote>
<h2 id="篇二：Random-Forest-Adaboost-GBDT算法"><a href="#篇二：Random-Forest-Adaboost-GBDT算法" class="headerlink" title="篇二：Random Forest, Adaboost, GBDT算法"></a>篇二：Random Forest, Adaboost, GBDT算法</h2><p>主要介绍基于集成学习的决策树，其主要通过不同学习框架生产基学习器（base learners），并综合所有基学习器的预测结果来改善单个基学习器的识别率和泛化性。</p>
<h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><p>常见的集成学习框架有三种：Bagging，Boosting 和 Stacking。</p>
<h4 id="1-1-Bagging"><a href="#1-1-Bagging" class="headerlink" title="1.1 Bagging"></a>1.1 Bagging</h4><p>Bagging 全称叫 Bootstrap aggregating，看到 Bootstrap 我们立刻想到著名的开源前端框架（抖个机灵，是 Bootstrap 抽样方法） ，每个基学习器都会对训练集进行<strong>有放回抽样</strong>得到子训练集，比较著名的采样法为 0.632 自助法。每个基学习器基于不同子训练集进行训练，并综合所有基学习器的预测值得到最终的预测结果。Bagging 常用的综合方法是投票法，票数最多的类别为预测类别。</p>
<p><img src="/2020/07/25/决策树/1595652338160.png" alt="1595652338160"></p>
<h4 id="1-2-Boosting"><a href="#1-2-Boosting" class="headerlink" title="1.2 Boosting"></a>1.2 Boosting</h4><p>Boosting 训练过程为阶梯状，<strong>基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习</strong>，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为加权法。</p>
<p><img src="/2020/07/25/决策树/1595652444294.png" alt="1595652444294"></p>
<h4 id="1-3-Stacking"><a href="#1-3-Stacking" class="headerlink" title="1.3 Stacking"></a>1.3 Stacking</h4><p>Stacking 是先用全部数据训练好基模型，然后每个基模型都对每个训练样本进行预测，其预测值将作为训练样本的特征值，最终会得到新的训练样本，然后基于新的训练样本进行训练得到模型，然后得到最终预测结果。</p>
<blockquote>
<p>为什么集成学习会好于单个学习器呢？原因可能有三：</p>
<ol>
<li>训练样本可能无法选择出最好的单个学习器，由于没法选择出最好的学习器，所以干脆结合起来一起用；</li>
<li>假设能找到最好的学习器，但由于算法运算的限制无法找到最优解，只能找到次优解，采用集成学习可以弥补算法的不足；</li>
<li>可能算法无法得到最优解，而集成学习能够得到近似解。比如说最优解是一条对角线，而单个决策树得到的结果只能是平行于坐标轴的，但是集成学习可以去拟合这条对角线。</li>
</ol>
</blockquote>
<h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><p>如何从偏差和方差的角度来理解集成学习。</p>
<h4 id="2-1-集成学习的偏差与方差"><a href="#2-1-集成学习的偏差与方差" class="headerlink" title="2.1 集成学习的偏差与方差"></a>2.1 集成学习的偏差与方差</h4><p><strong>偏差（Bias）描述的是预测值和真实值之差；方差（Variance）描述的是预测值作为随机变量的离散程度。</strong>放一场很经典的图：</p>
<p><img src="/2020/07/25/决策树/1595652870653.png" alt="1595652870653"></p>
<p>模型的偏差与方差</p>
<ul>
<li><strong>偏差</strong>：描述样本拟合出的模型的预测结果的期望与样本真实结果的差距，要想偏差表现的好，就需要复杂化模型，增加模型的参数，但这样容易过拟合，过拟合对应上图的 High Variance，点会很分散。低偏差对应的点都打在靶心附近，所以喵的很准，但不一定很稳；</li>
<li><strong>方差</strong>：描述样本上训练出来的模型在测试集上的表现，要想方差表现的好，需要简化模型，减少模型的复杂度，但这样容易欠拟合，欠拟合对应上图 High Bias，点偏离中心。低方差对应就是点都打的很集中，但不一定是靶心附近，手很稳，但不一定瞄的准。</li>
</ul>
<p>我们常说集成学习中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。<strong>Bagging 和 Stacking 中的基模型为强模型（偏差低方差高），Boosting 中的基模型为弱模型</strong>。</p>
<p><img src="/2020/07/25/决策树/1595653661803.png" alt="1595653661803"></p>
<h4 id="2-2-Bagging-的偏差与方差"><a href="#2-2-Bagging-的偏差与方差" class="headerlink" title="2.2 Bagging 的偏差与方差"></a>2.2 Bagging 的偏差与方差</h4><p><img src="/2020/07/25/决策树/1595653718100.png" alt="1595653718100"></p>
<p>在此我们知道了为什么 Bagging 中的基模型一定要为强模型，如果 Bagging 使用弱模型则会导致整体模型的偏差提高，而准确度降低。</p>
<p>Random Forest 是经典的基于 Bagging 框架的模型，并在此基础上通过引入特征采样和样本采样来降低基模型间的相关性，在公式中显著降低方差公式中的第二项，略微升高第一项，从而使得整体降低模型整体方差。</p>
<h4 id="2-3-Boosting-的偏差与方差"><a href="#2-3-Boosting-的偏差与方差" class="headerlink" title="2.3 Boosting 的偏差与方差"></a>2.3 Boosting 的偏差与方差</h4><p><img src="/2020/07/25/决策树/1595653801559.png" alt="1595653801559"></p>
<p>基于 Boosting 框架的 Gradient Boosting Decision Tree 模型中基模型也为树模型，同 Random Forest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<h4 id="2-4-小结"><a href="#2-4-小结" class="headerlink" title="2.4 小结"></a>2.4 小结</h4><ul>
<li>我们可以使用模型的偏差和方差来近似描述模型的准确度；</li>
<li>对于 Bagging 来说，整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型；</li>
<li>对于 Boosting 来说，整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型。</li>
</ul>
<h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p>RF 算法由很多决策树组成，每一棵决策树之间没有关联。建立完森林后，当有新样本进入时，每棵决策树都会分别进行判断，然后基于投票法给出分类结果。</p>
<h4 id="3-1-思想-1"><a href="#3-1-思想-1" class="headerlink" title="3.1 思想"></a>3.1 思想</h4><p>Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：</p>
<ol>
<li>随机选择样本（有放回抽样）；</li>
<li>随机选择特征；</li>
<li>构建决策树；</li>
<li>随机森林投票（平均）。</li>
</ol>
<p>随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；随机选择特征是指在每个节点在分裂过程中都是随机选择特征的（区别与每棵树随机选择一批特征）。</p>
<p>这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。</p>
<p>随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算不剪枝也不会出现过拟合。</p>
<h4 id="3-2-优缺点"><a href="#3-2-优缺点" class="headerlink" title="3.2 优缺点"></a>3.2 优缺点</h4><p>优点</p>
<ol>
<li>在数据集上表现良好，相对于其他算法有较大的优势</li>
<li>易于并行化，在大数据集上有很大的优势；</li>
<li>能够处理高维度数据，不用做特征选择。</li>
</ol>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>AdaBoost（Adaptive Boosting，自适应增强），其自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。</p>
<h4 id="4-1-思想"><a href="#4-1-思想" class="headerlink" title="4.1 思想"></a>4.1 思想</h4><p>Adaboost 迭代算法有三步：</p>
<ol>
<li>初始化训练样本的权值分布，每个样本具有相同权重；</li>
<li>训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；</li>
<li>将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。</li>
</ol>
<h4 id="4-2-细节"><a href="#4-2-细节" class="headerlink" title="4.2 细节"></a>4.2 细节</h4><h5 id="4-2-1-损失函数"><a href="#4-2-1-损失函数" class="headerlink" title="4.2.1 损失函数"></a>4.2.1 损失函数</h5><p><img src="/2020/07/25/决策树/1595663386859.png" alt="1595663386859"></p>
<p><img src="/2020/07/25/决策树/1595663456133.png" alt="1595663456133"></p>
<p><img src="/2020/07/25/决策树/1595663508910.png" alt="1595663508910"></p>
<h5 id="4-2-2-正则化"><a href="#4-2-2-正则化" class="headerlink" title="4.2.2 正则化"></a>4.2.2 正则化</h5><p><img src="/2020/07/25/决策树/1595663617670.png" alt="1595663617670"></p>
<h4 id="4-3-优缺点"><a href="#4-3-优缺点" class="headerlink" title="4.3 优缺点"></a>4.3 优缺点</h4><p><strong>优点</strong></p>
<ol>
<li>分类精度高；</li>
<li>可以用各种回归分类模型来构建弱学习器，非常灵活；</li>
<li>不容易发生过拟合。</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>对异常点敏感，异常点会获得较高权重。</li>
</ol>
<p>另：</p>
<p><strong>优点</strong> </p>
<p>（1）Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也不存在过拟合的现象。 </p>
<p>（2）Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。 </p>
<p>（3）Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。 </p>
<p>（4）Adaboost可以在不改变训练数据，只改变数据权值分布，使得数据在不同学习器中产生不同作用，类似于重采样。 </p>
<p><strong>缺点</strong> </p>
<p>​     在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>GBDT（Gradient Boosting Decision Tree）是一种迭代的决策树算法，该算法由多棵决策树组成，从名字中我们可以看出来它是属于 Boosting 策略。GBDT 是被公认的泛化能力较强的算法。</p>
<h4 id="5-1-思想"><a href="#5-1-思想" class="headerlink" title="5.1 思想"></a>5.1 思想</h4><p>GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和SHringkage（一个重要演变）</p>
<h5 id="5-1-1-回归树（Regression-Decision-Tree）"><a href="#5-1-1-回归树（Regression-Decision-Tree）" class="headerlink" title="5.1.1 回归树（Regression Decision Tree）"></a>5.1.1 回归树（Regression Decision Tree）</h5><p>如果认为 GBDT 由很多分类树那就大错特错了（虽然调整后也可以分类）。对于分类树而言，其值加减无意义（如性别），而对于回归树而言，其值加减才是有意义的（如说年龄）。<strong>GBDT 的核心在于累加所有树的结果作为最终结果</strong>，所以 <strong><span style="color:red">GBDT 中的树都是回归树</span></strong>，不是分类树，这一点相当重要。</p>
<p>回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。</p>
<h5 id="5-1-2-梯度迭代（Gradient-Boosting）"><a href="#5-1-2-梯度迭代（Gradient-Boosting）" class="headerlink" title="5.1.2 梯度迭代（Gradient Boosting）"></a>5.1.2 梯度迭代（Gradient Boosting）</h5><p>上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，<strong>GBDT 的每一棵树都是以之前树得到的残差来更新目标值，这样每一棵树的值加起来即为 GBDT 的预测值</strong>。</p>
<p><img src="/2020/07/25/决策树/1595667008154.png" alt="1595667008154"></p>
<blockquote>
<p>举个例子：比如说 A 用户年龄 20 岁，第一棵树预测 12 岁，那么残差就是 8，第二棵树用 8 来学习，假设其预测为 5，那么其残差即为 3，如此继续学习即可。</p>
</blockquote>
<p><img src="/2020/07/25/决策树/1595667099649.png" alt="1595667099649"></p>
<p><img src="/2020/07/25/决策树/1595667123278.png" alt="1595667123278"></p>
<p><img src="/2020/07/25/决策树/1595667175530.png" alt="1595667175530"></p>
<p>GBDT 的 Boosting 不同于 Adaboost 的 Boosting，GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对与分对样本的权重趋于 0，这样后面的树就能专注于那些被分错的样本。</p>
<h5 id="5-1-13-缩减（Shrinkage）"><a href="#5-1-13-缩减（Shrinkage）" class="headerlink" title="5.1.13 缩减（Shrinkage）"></a>5.1.13 缩减（Shrinkage）</h5><p>Shrinkage 的思想认为，每走一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它并不是完全信任每一棵残差树。</p>
<p><img src="/2020/07/25/决策树/1595667249030.png" alt="1595667249030"></p>
<p>Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大。</p>
<h4 id="5-2-优缺点"><a href="#5-2-优缺点" class="headerlink" title="5.2 优缺点"></a>5.2 优缺点</h4><p><strong>优点</strong></p>
<ol>
<li>可以自动进行特征组合，拟合非线性数据；</li>
<li>可以灵活处理各种类型的数据。</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>对异常点敏感。</li>
</ol>
<p>另：</p>
<p><img src="/2020/07/25/决策树/1595669502617.png" alt="1595669502617"></p>
<p><img src="/2020/07/25/决策树/1595669541744.png" alt="1595669541744"></p>
<h4 id="5-3-与-Adaboost-的对比"><a href="#5-3-与-Adaboost-的对比" class="headerlink" title="5.3 与 Adaboost 的对比"></a>5.3 与 Adaboost 的对比</h4><p>相同：</p>
<ol>
<li>都是 Boosting 家族成员，使用弱分类器；</li>
<li>都使用前向分布算法；</li>
</ol>
<p>不同：</p>
<ol>
<li>迭代思路不同：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）；</li>
<li>损失函数不同：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；</li>
</ol>
<hr>
<blockquote>
<p>以下内容来源于公众号Datawhale</p>
<p><a href="https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ</a></p>
</blockquote>
<h2 id="篇三：XGBoost"><a href="#篇三：XGBoost" class="headerlink" title="篇三：XGBoost"></a>篇三：XGBoost</h2><p><img src="/2020/07/25/决策树/1595667573239.png" alt="1595667573239"></p>
<p>（注：以下内容只包含XGBoost部分，LightGBM部分可见原文链接）</p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost 是大规模并行 boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包，比常见的工具包快 10 倍以上。Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是目标函数的定义。</p>
<h4 id="1-1-数学原理"><a href="#1-1-数学原理" class="headerlink" title="1.1 数学原理"></a>1.1 数学原理</h4><h5 id="1-1-1-目标函数"><a href="#1-1-1-目标函数" class="headerlink" title="1.1.1 目标函数"></a>1.1.1 目标函数</h5><p><img src="/2020/07/25/决策树/1595667719081.png" alt="1595667719081"></p>
<p><img src="/2020/07/25/决策树/1595667791401.png" alt="1595667791401"></p>
<p><img src="/2020/07/25/决策树/1595667837751.png" alt="1595667837751"></p>
<p><img src="/2020/07/25/决策树/1595667861205.png" alt="1595667861205"></p>
<p>$g_i=\frac{\partial\ l(y_i,\hat{y_i}^{(t-1)})}{\partial\,\hat{y_i}^{(t-1)}}$, $h_i=\frac{\partial^2 l(y_i,\hat{y_i}^{(t-1)})}{\partial^2 \hat{y_i}^{(t-1)}}$</p>
<p><img src="/2020/07/25/决策树/1595667888425.png" alt="1595667888425"></p>
<p>又因为有 </p>
<script type="math/tex; mode=display">
\sum_{i=1}^{t} \Omega(f_i)=\Omega(f_t)+\sum_{i=1}^{t-1}\Omega(f_i)</script><p>并且$\sum_{i=1}^{t-1}\Omega(f_i)$在前t-1棵树已知下为常数，所以目标函数可写为：</p>
<script type="math/tex; mode=display">
Obj^{(t)}≈\sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)</script><h5 id="1-1-2-基于决策树的目标函数"><a href="#1-1-2-基于决策树的目标函数" class="headerlink" title="1.1.2 基于决策树的目标函数"></a>1.1.2 基于决策树的目标函数</h5><p>我们知道 Xgboost 的基模型不仅支持决策树，还支持线性模型，这里我们主要介绍基于决策树的目标函数。</p>
<p><img src="/2020/07/25/决策树/1595667950419.png" alt="1595667950419"></p>
<hr>
<p>另一种描述：</p>
<blockquote>
<p>参考 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
</blockquote>
<p>现在定义$\Omega(f_t)$：假设我们待训练的第t棵树有T个叶子结点，叶子结点的输出向量表示如下：</p>
<script type="math/tex; mode=display">
[w_1,w_2,...,w_T]</script><p>假设 $q(x):R^d \rightarrow \{1,2,3,…,T\}$ 表示样本到叶子结点的映射，那么$f_t(x)=w_{q(x)},w∈R^T$.那么我们定义：</p>
<script type="math/tex; mode=display">
\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2</script><p>其中$T$为叶子结点数，$w_j$为叶子结点$j$的输出，$\gamma$为系数。</p>
<hr>
<p><img src="/2020/07/25/决策树/1595667972342.png" alt="1595667972342"></p>
<p><img src="/2020/07/25/决策树/1595667995025.png" alt="1595667995025"></p>
<p><img src="/2020/07/25/决策树/1595668020156.png" alt="1595668020156"></p>
<hr>
<p>另一种描述：</p>
<blockquote>
<p>参考 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
</blockquote>
<p><img src="/2020/07/25/决策树/1595822346216.png" alt="1595822346216"></p>
<hr>
<p><img src="/2020/07/25/决策树/1595668057942.png" alt="1595668057942"></p>
<h5 id="1-1-3-最优切分点划分算法"><a href="#1-1-3-最优切分点划分算法" class="headerlink" title="1.1.3 最优切分点划分算法"></a>1.1.3 最优切分点划分算法</h5><p>我们刚刚的假设前提是已知前t-1棵树，现在我们讨论怎么生成树。根据决策树的生成策略，在每次分裂节点的时候我们需要考虑能使得损失函数减小最快的节点，也就是分裂后损失函数减去分裂前损失函数我们称之为Gain：</p>
<p><img src="/2020/07/25/决策树/1595822888787.png" alt="1595822888787"></p>
<p>Gain越大越能说明分裂后目标函数值减小越多。</p>
<p>如何找到叶子的节点的最优切分点：Xgboost 支持两种分裂节点的方法——贪心算法和近似算法。</p>
<h6 id="1）贪心算法-精确贪心算法-Basic-Exact-Greedy-Algorithm"><a href="#1）贪心算法-精确贪心算法-Basic-Exact-Greedy-Algorithm" class="headerlink" title="1）贪心算法 (精确贪心算法 Basic Exact Greedy Algorithm)"></a>1）贪心算法 (精确贪心算法 Basic Exact Greedy Algorithm)</h6><p>在决策树（CART）里面，我们使用的是精确贪心算法，也就是将所有特征的所有取值排序（耗时耗内存巨大），然后比较每一个点的Gini，找出变化最大的节点。当特征是连续特征时，我们对连续值离散化，取两点的平均值为分割节点。可以看到，这里的排序算法需要花费大量的时间，因为要遍历整个样本所有特征，而且还要排序。</p>
<p>步骤：</p>
<ol>
<li>从深度为 0 的树开始，对每个叶节点枚举所有的可用特征；</li>
<li>针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；</li>
<li>选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集</li>
<li>回到第 1 步，递归执行到满足特定条件为止</li>
</ol>
<p>那么如何计算每个特征的分裂收益呢？</p>
<p><img src="/2020/07/25/决策树/1595668137245.png" alt="1595668137245"></p>
<p><img src="/2020/07/25/决策树/1595668154967.png" alt="1595668154967"></p>
<p><img src="/2020/07/25/决策树/1595823332714.png" alt="1595823332714"></p>
<h6 id="2）近似算法-Approximate-Algorithm"><a href="#2）近似算法-Approximate-Algorithm" class="headerlink" title="2）近似算法 (Approximate Algorithm)"></a>2）近似算法 (Approximate Algorithm)</h6><p>贪婪算法可以得到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪婪算法这一缺点给出了近似最优解。</p>
<p>对于每个特征，只考察分位点可以减少计算复杂度。</p>
<p>该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。</p>
<hr>
<p>另一种描述：</p>
<blockquote>
<p>参考 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
</blockquote>
<p>该算法首先根据特征分布的百分位数(percentiles)提出候选分裂点，将连续特征映射到由这些候选点分割的桶中，汇总统计信息并根据汇总的信息在提案中找到最佳解决方案。对于某个特征k，算法首先根据特征分布的分位数找到特征切割点的候选集合$S_k=\{S_{k_1},S_{k_2},…,S_{k_l}\}$ , 然后将特征k的值根据集合$S_k$ 划分到桶(bucket)中，接着对每个桶内的样本统计值G、H进行累加，最后在这些累计的统计量上寻找最佳分裂点。</p>
<hr>
<p>在提出候选切分点时有两种策略：</p>
<ul>
<li>Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；</li>
<li>Local：每次分裂前将重新提出候选切分点。</li>
</ul>
<p>直观上来看，Local 策略需要更多的计算步骤，而 Global 策略因为节点没有划分所以需要更多的候选点。</p>
<p>下图给出不同种分裂策略的 AUC 变换曲线，横坐标为迭代次数，纵坐标为测试集 AUC，eps 为近似算法的精度，其倒数为桶的数量。</p>
<p><img src="/2020/07/25/决策树/1595668243451.png" alt="1595668243451"></p>
<p>我们可以看到 Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有相似的精度。此外我们还发现，在 eps 取值合理的情况下，分位数策略可以获得与贪婪算法相同的精度。</p>
<p><img src="/2020/07/25/决策树/1595668320876.png" alt="1595668320876"></p>
<p><img src="/2020/07/25/决策树/1595668340241.png" alt="1595668340241"></p>
<h5 id="1-1-4-加权分位数缩略图"><a href="#1-1-4-加权分位数缩略图" class="headerlink" title="1.1.4 加权分位数缩略图"></a>1.1.4 加权分位数缩略图</h5><p><img src="/2020/07/25/决策树/1595668403168.png" alt="1595668403168"></p>
<p><img src="/2020/07/25/决策树/1595668446774.png" alt="1595668446774"></p>
<h5 id="1-1-5-稀疏感知算法"><a href="#1-1-5-稀疏感知算法" class="headerlink" title="1.1.5 稀疏感知算法"></a>1.1.5 稀疏感知算法</h5><p>在决策树的第一篇文章中我们介绍 CART 树在应对数据缺失时的分裂策略，XGBoost 也给出了其解决方案。</p>
<p>XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。</p>
<p>在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了特征未缺失的样本遍历，而特征值缺失的样本无需遍历只需直接分配到左右节点，故算法所需遍历的样本量减少，下图可以看到稀疏感知算法比 basic 算法速度块了超过 50 倍。</p>
<p><img src="/2020/07/25/决策树/1595668537184.png" alt="1595668537184"></p>
<h4 id="1-2-工程实现"><a href="#1-2-工程实现" class="headerlink" title="1.2 工程实现"></a>1.2 工程实现</h4><h5 id="1-2-1-块结构设计"><a href="#1-2-1-块结构设计" class="headerlink" title="1.2.1 块结构设计"></a>1.2.1 块结构设计</h5><p>我们知道，决策树的学习最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序。而 XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。</p>
<ul>
<li>每一个块结构包括一个或多个已经排序好的特征；</li>
<li>缺失特征值将不进行排序；</li>
<li>每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；</li>
</ul>
<p><img src="/2020/07/25/决策树/1595668680481.png" alt="1595668680481"></p>
<p>这种块结构存储的特征之间相互独立，方便计算机进行并行计算。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 Xgboost 能够实现分布式或者多线程计算的原因。</p>
<h5 id="1-2-2-缓存访问优化算法"><a href="#1-2-2-缓存访问优化算法" class="headerlink" title="1.2.2 缓存访问优化算法"></a>1.2.2 缓存访问优化算法</h5><p>块结构的设计可以减少节点分裂时的计算量，但特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续，这样会造成缓存命中率低，从而影响到算法的效率。</p>
<p>为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。</p>
<p>此外适当调整块大小，也可以有助于缓存优化。</p>
<h5 id="1-2-3-“核外”块计算"><a href="#1-2-3-“核外”块计算" class="headerlink" title="1.2.3 “核外”块计算"></a>1.2.3 “核外”块计算</h5><p>当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。为了解决这个问题，XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行。</p>
<p>此外，XGBoost 还用了两种方法来降低硬盘读写的开销：</p>
<ul>
<li>块压缩：对 Block 进行按列压缩，并在读取时进行解压；</li>
<li>块拆分：将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。</li>
</ul>
<h4 id="1-3-优缺点"><a href="#1-3-优缺点" class="headerlink" title="1.3 优缺点"></a>1.3 优缺点</h4><h5 id="1-3-1-优点"><a href="#1-3-1-优点" class="headerlink" title="1.3.1 优点"></a>1.3.1 优点</h5><ol>
<li>精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost  对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li>
<li>灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的Logistic回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li>正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；</li>
<li>Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；</li>
<li>列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；</li>
<li>缺失值处理：XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；</li>
<li>可以并行化操作：块结构可以很好的支持并行计算。</li>
</ol>
<h5 id="1-3-2-缺点"><a href="#1-3-2-缺点" class="headerlink" title="1.3.2 缺点"></a>1.3.2 缺点</h5><ol>
<li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；</li>
<li>预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。</li>
</ol>
<h3 id="XGBoost动手实践"><a href="#XGBoost动手实践" class="headerlink" title="XGBoost动手实践"></a>XGBoost动手实践</h3><p>本部分内容来源于 <a href="https://mp.weixin.qq.com/s/AAKPSIHk1iUqCeUibrORqQ" target="_blank" rel="noopener">我的XGBoost学习经历及动手实践</a></p>
<h4 id="1-引入基本工具库"><a href="#1-引入基本工具库" class="headerlink" title="1. 引入基本工具库"></a>1. 引入基本工具库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入基本工具库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">"ggplot"</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h4 id="2-XGBoost原生工具库的上手"><a href="#2-XGBoost原生工具库的上手" class="headerlink" title="2. XGBoost原生工具库的上手"></a>2. XGBoost原生工具库的上手</h4><p>更详细的参数设置见后面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb  <span class="comment"># 引入工具库</span></span><br><span class="line"><span class="comment"># read in data</span></span><br><span class="line"><span class="comment"># XGBoost的专属数据格式，但是也可以用dataframe或者ndarray</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.train'</span>)   </span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.test'</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># specify parameters via map</span></span><br><span class="line"><span class="comment"># 设置XGB的参数，使用字典形式传入</span></span><br><span class="line">param = &#123;<span class="string">'max_depth'</span>:<span class="number">2</span>, <span class="string">'eta'</span>:<span class="number">1</span>, <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span> &#125;    </span><br><span class="line"></span><br><span class="line">num_round = <span class="number">2</span>     <span class="comment"># 使用线程数</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)   <span class="comment"># 训练</span></span><br><span class="line"><span class="comment"># make prediction</span></span><br><span class="line">preds = bst.predict(dtest)   <span class="comment"># 预测</span></span><br></pre></td></tr></table></figure>
<h4 id="3-XGBoost的参数设置"><a href="#3-XGBoost的参数设置" class="headerlink" title="3. XGBoost的参数设置"></a>3. XGBoost的参数设置</h4><p>(括号内的名称为sklearn接口对应的参数名字)</p>
<p><span style="background:yellow">更详细的介绍见 <a href="https://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="noopener">官方文档</a>.</span></p>
<p>XGBoost的参数分为三种：</p>
<h5 id="3-1-通用参数"><a href="#3-1-通用参数" class="headerlink" title="3.1. 通用参数"></a>3.1. 通用参数</h5><blockquote>
<ul>
<li>booster: 使用哪个弱学习器训练，默认gbtree，可选gbtree，gblinear或 dart<ul>
<li>booster参数：控制每一步的booster (tree/regression). 因为tree的性能比线性回归好得多，因此我们很少用线性回归. <code>gbtree</code> and <code>dart</code> use tree based models while <code>gblinear</code> uses linear functions.</li>
</ul>
</li>
<li>nthread：用于运行XGBoost的并行线程数，默认为最大可用线程数</li>
<li>verbosity：打印消息的详细程度。默认值为1，有效值为0（静默），1（警告），2（信息），3（调试）。</li>
</ul>
</blockquote>
<p>tree booster：</p>
<blockquote>
<ul>
<li><p>Tree Booster的参数：</p>
</li>
<li><ul>
<li><p>eta（learning_rate）：learning_rate，在更新中使用步长收缩以防止过度拟合，默认= 0.3，范围：[0,1]；典型值一般设置为：0.01-0.2</p>
</li>
<li><p>gamma（min_split_loss）：默认= 0，分裂节点时，损失函数减小值只有大于等于gamma节点才分裂，gamma值越大，算法越保守，越不容易过拟合，但性能就不一定能保证，需要平衡。范围：[0，∞]</p>
</li>
<li><p>max_depth：默认= 6，一棵树的最大深度。增加此值将使模型更复杂，并且更可能过度拟合。范围：[0，∞]</p>
</li>
<li><p>min_child_weight：默认值= 1，如果新分裂的节点的样本权重和小于min_child_weight则停止分裂 。这个可以用来减少过拟合，但是也不能太高，会导致欠拟合。范围：[0，∞]</p>
</li>
<li><p>max_delta_step：默认= 0，允许每个叶子输出的最大增量步长。如果将该值设置为0，则表示没有约束。如果将其设置为正值，则可以帮助使更新步骤更加保守。通常不需要此参数，但是当类极度不平衡时，它可能有助于逻辑回归。将其设置为1-10的值可能有助于控制更新。范围：[0，∞]</p>
</li>
<li><p>subsample：默认值= 1，构建每棵树对样本的采样率，如果设置成0.5，XGBoost会随机选择一半的样本作为训练集。范围：（0,1]</p>
</li>
<li><p>sampling_method：默认= uniform，用于对训练实例进行采样的方法。</p>
</li>
<li><ul>
<li>uniform：每个训练实例的选择概率均等。通常将subsample&gt; = 0.5 设置 为良好的效果。</li>
<li>gradient_based：每个训练实例的选择概率与规则化的梯度绝对值成正比，具体来说就是 $\sqrt{g^2+\lambda h^2}$，subsample可以设置为低至0.1，而不会损失模型精度。</li>
</ul>
</li>
<li><p>colsample_bytree：默认= 1，列采样率，也就是特征采样率。范围为（0，1]</p>
</li>
<li><p>lambda（reg_lambda）：默认=1，L2正则化权重项。增加此值将使模型更加保守。</p>
</li>
<li><p>alpha（reg_alpha）：默认= 0，权重的L1正则化项。增加此值将使模型更加保守。</p>
</li>
<li><p>tree_method：默认=auto，XGBoost中使用的树构建算法。可选：<code>auto</code>, <code>exact</code>, <code>approx</code>, <code>hist</code>, <code>gpu_hist</code></p>
</li>
<li><ul>
<li>auto：使用启发式选择最快的方法。</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>对于小型数据集，将使用精确贪婪<code>exact</code>。</li>
<li>对于较大的数据集，将选择近似算法<code>approx</code>。建议尝试<code>hist</code>，<code>gpu_hist</code>，对于大量的数据有更高的性能。<code>gpu_hist</code>支持 external memory 外部存储器。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>exact：精确的贪婪算法。枚举所有拆分的候选点。</li>
<li>approx：使用分位数和梯度直方图的近似贪婪算法。</li>
<li>hist：更快的直方图优化的近似贪婪算法。（LightGBM也是使用直方图算法）</li>
<li>gpu_hist：GPU hist算法的实现。</li>
</ul>
</li>
<li><p>scale_pos_weight: 默认值1，控制正负权重的平衡，这对于不平衡的类别很有用。Kaggle竞赛一般设置 sum(negative instances) / sum(positive instances)，在类别高度不平衡的情况下，将参数设置大于0，可以加快收敛。</p>
</li>
<li><p>num_parallel_tree：默认=1，每次迭代期间构造的并行树的数量。此选项用于支持增强型随机森林。</p>
</li>
<li><p>monotone_constraints：可变单调性的约束，在某些情况下，如果有非常强烈的先验信念认为真实的关系具有一定的质量，则可以使用约束条件来提高模型的预测性能。（例如params_constrained[‘monotone_constraints’] = “(1,-1)”，(1,-1)告诉XGBoost对第一个预测变量施加增加的约束，对第二个预测变量施加减小的约束。）</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<p>linear booster：</p>
<blockquote>
<ul>
<li><p>Linear Booster的参数：</p>
</li>
<li><ul>
<li><p>lambda（reg_lambda）：默认= 0，L2正则化权重项。增加此值将使模型更加保守。归一化为训练示例数。(Normalised to number of training examples.)</p>
</li>
<li><p>alpha（reg_alpha）：默认= 0，权重的L1正则化项。增加此值将使模型更加保守。归一化为训练示例数。</p>
</li>
<li><p>updater：默认= shotgun。</p>
</li>
<li><ul>
<li>shotgun：基于shotgun算法的平行坐标下降算法。使用“ hogwild”并行性，因此每次运行都产生不确定的解决方案。</li>
<li>coord_descent：普通坐标下降算法。同样是多线程的，但仍会产生确定性的解决方案。</li>
</ul>
</li>
<li><p>feature_selector：默认= cyclic。特征选择和排序方法</p>
</li>
<li><ul>
<li>cyclic：通过每次循环一个特征来实现的。</li>
<li>shuffle：类似于cyclic，但是在每次更新之前都有随机的特征变换。</li>
<li>random：一个随机(有放回)特征选择器。</li>
<li>greedy：选择梯度最大的特征。（贪婪选择）</li>
<li>thrifty：近似贪婪特征选择（近似于greedy）</li>
</ul>
</li>
<li><p>top_k：默认值为0，要选择的最重要特征数（在greedy和thrifty内）。The number of top features to select in <code>greedy</code> and <code>thrifty</code> feature selector. The value of 0 means using all the features.</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<h5 id="3-2-任务参数"><a href="#3-2-任务参数" class="headerlink" title="3.2. 任务参数"></a>3.2. 任务参数</h5><p>学习目标参数。这个参数用来控制理想的优化目标和每一步结果的度量方法。</p>
<blockquote>
<ul>
<li><p>objective：默认=<code>reg:squarederror</code>，表示最小平方误差。</p>
</li>
<li><ul>
<li><p><code>reg:squarederror</code>：最小平方误差。</p>
</li>
<li><p><code>reg:squaredlogerror</code>：对数平方损失。</p>
</li>
<li><p><code>reg:logistic</code>：逻辑回归</p>
</li>
<li><p><code>reg:pseudohubererror</code>： 使用伪Huber损失进行回归，这是绝对损失的两倍可微选择。</p>
</li>
<li><p><code>binary:logistic</code>：logistic regression for binary classification, output probability.</p>
</li>
<li><p><code>binary:logitraw</code>：logistic regression for binary classification, output score before logistic transformation.</p>
</li>
<li><p><code>binary:hinge</code>：二元分类的铰链损失(hinge loss)。这使预测为0或1，而不是产生概率。（SVM就是铰链损失函数）</p>
</li>
<li><p><code>count:poisson</code>：计数数据的泊松回归，输出泊松分布的平均值。</p>
</li>
<li><p><code>survival:cox</code>：针对正确的（right censored）生存时间数据进行Cox回归（负值被视为正确的生存时间）。</p>
</li>
<li><p><code>survival:aft</code>：用于检查生存时间数据的加速故障时间模型。</p>
</li>
<li><p><code>aft_loss_distribution</code>：Probabilty Density Function used by <code>survival:aft</code> objective and <code>aft-nloglik</code> metric.</p>
</li>
<li><p><code>multi:softmax</code>：设置XGBoost以使用softmax目标进行多类分类，还需要设置num_class（类数）</p>
</li>
<li><p><code>multi:softprob</code>：与softmax相同，但输出向量，可以进一步重整为矩阵。结果包含属于每个类别的每个数据点的预测概率。</p>
</li>
<li><p><code>rank:pairwise</code>：使用LambdaMART进行成对排名，从而使成对损失最小化。</p>
</li>
<li><p><code>rank:ndcg</code>：使用LambdaMART进行列表式排名，使标准化折让累积收益（NDCG）最大化。</p>
</li>
<li><p><code>rank:map</code>：使用LambdaMART进行列表平均排名，使平均平均精度（MAP）最大化。</p>
</li>
<li><p><code>reg:gamma</code>：使用对数链接(log-link)进行伽马回归。输出是伽马分布的平均值。</p>
</li>
<li><p><code>reg:tweedie</code>：使用对数链接进行Tweedie回归。</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>eval_metric：验证数据的评估指标，将根据objective分配默认指标 (rmse for regression, and error for classification, mean average precision for ranking)，用户可以添加多个评估指标 (Python users: remember to pass the metrics in as list of parameters pairs instead of map)</p>
</li>
<li><ul>
<li><p>rmse，均方根误差；</p>
</li>
<li><p>rmsle：均方根对数误差；Default metric of <code>reg:squaredlogerror</code> objective.</p>
</li>
<li><p>mae：平均绝对误差；</p>
</li>
<li><p>mphe：平均伪Huber错误；</p>
</li>
<li><p>logloss：负对数似然；</p>
</li>
<li><p>error：二元分类错误率； It is calculated as <code>#(wrong cases)/#(all cases)</code></p>
</li>
<li><p>merror：多类分类错误率；</p>
</li>
<li><p>mlogloss：多类logloss；</p>
</li>
<li><p>auc：曲线下面积；</p>
</li>
<li><p>aucpr：PR曲线下的面积；</p>
</li>
<li><p>ndcg：归一化累计折扣；</p>
</li>
<li><p>map：平均精度；</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>seed ：随机数种子，[默认= 0]。</li>
</ul>
</blockquote>
<h5 id="3-3-命令行参数"><a href="#3-3-命令行参数" class="headerlink" title="3.3. 命令行参数"></a>3.3. 命令行参数</h5><p>这里不说了，因为很少用命令行控制台版本。(only used in the console version of XGBoost)</p>
<h4 id="4-XGBoost的调参说明"><a href="#4-XGBoost的调参说明" class="headerlink" title="4. XGBoost的调参说明"></a>4. XGBoost的调参说明</h4><p>参数调优的一般步骤：</p>
<ul>
<li>1.确定（较大）学习速率和提升参数调优的初始值</li>
<li>2.max_depth 和 min_child_weight 参数调优</li>
<li>3.gamma参数调优</li>
<li>4.subsample 和 colsample_bytree 参数调优</li>
<li>5.正则化参数alpha调优</li>
<li>6.降低学习速率和使用更多的决策树</li>
</ul>
<h4 id="5-XGBoost详细攻略"><a href="#5-XGBoost详细攻略" class="headerlink" title="5. XGBoost详细攻略"></a>5. XGBoost详细攻略</h4><p>更详细内容见<a href="https://xgboost.readthedocs.io/en/latest/python/python_intro.html" target="_blank" rel="noopener">官方文档</a></p>
<h5 id="1-安装XGBoost"><a href="#1-安装XGBoost" class="headerlink" title="1). 安装XGBoost"></a>1). 安装XGBoost</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip3 install xgboost</span><br><span class="line">或</span><br><span class="line">pip install xgboost</span><br><span class="line"></span><br><span class="line">更新</span><br><span class="line">pip install --upgrade xgboost</span><br></pre></td></tr></table></figure>
<h5 id="2-数据接口（XGBoost可处理的数据格式DMatrix）"><a href="#2-数据接口（XGBoost可处理的数据格式DMatrix）" class="headerlink" title="2). 数据接口（XGBoost可处理的数据格式DMatrix）"></a>2). 数据接口（XGBoost可处理的数据格式DMatrix）</h5><p>The XGBoost python module is able to load data from:</p>
<ul>
<li>LibSVM text format file</li>
<li>Comma-separated values (CSV) file</li>
<li>NumPy 2D array</li>
<li>SciPy 2D sparse array</li>
<li>cuDF DataFrame</li>
<li>Pandas data frame, and</li>
<li>XGBoost binary buffer file.</li>
</ul>
<p>(See <a href="https://xgboost.readthedocs.io/en/latest/tutorials/input_format.html" target="_blank" rel="noopener">Text Input Format of DMatrix</a> for detailed description of text input format.)</p>
<p>The data is stored in a <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix" target="_blank" rel="noopener"><code>DMatrix</code></a> object.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.LibSVM文本格式文件</span></span><br><span class="line"><span class="comment"># To load a libsvm text file or a XGBoost binary file into DMatrix:</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'train.svm.txt'</span>)</span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'test.svm.buffer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.CSV文件(不能含类别文本变量，如果存在文本变量请做特征处理如one-hot)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Categorical features not supported</span></span><br><span class="line"><span class="string">Note that XGBoost does not provide specialization for categorical features; if your data contains categorical features, load it as a NumPy array first and then perform corresponding preprocessing steps like one-hot encoding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Use Pandas to load CSV files with headers</span></span><br><span class="line"><span class="string">Currently, the DMLC data parser cannot parse CSV files with headers. Use Pandas (see below) to read CSV files with headers.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'train.csv?format=csv&amp;label_column=0'</span>)</span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'test.csv?format=csv&amp;label_column=0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.NumPy数组</span></span><br><span class="line">data = np.random.rand(<span class="number">5</span>, <span class="number">10</span>)  <span class="comment"># 5 entities, each contains 10 features</span></span><br><span class="line">label = np.random.randint(<span class="number">2</span>, size=<span class="number">5</span>)  <span class="comment"># binary target</span></span><br><span class="line">dtrain = xgb.DMatrix(data, label=label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.scipy.sparse数组</span></span><br><span class="line">csr = scipy.sparse.csr_matrix((dat, (row, col)))</span><br><span class="line">dtrain = xgb.DMatrix(csr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.pandas数据框dataframe</span></span><br><span class="line">data = pandas.DataFrame(np.arange(<span class="number">12</span>).reshape((<span class="number">4</span>,<span class="number">3</span>)), columns=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</span><br><span class="line">label = pandas.DataFrame(np.random.randint(<span class="number">2</span>, size=<span class="number">4</span>))</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label)</span><br></pre></td></tr></table></figure>
<p>笔者推荐：先保存到XGBoost二进制文件中将使加载速度更快，然后再加载进来</p>
<p>Saving <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix" target="_blank" rel="noopener"><code>DMatrix</code></a> into a XGBoost binary file will make loading faster:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.保存DMatrix到XGBoost二进制文件中</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'train.svm.txt'</span>)</span><br><span class="line">dtrain.save_binary(<span class="string">'train.buffer'</span>)</span><br><span class="line"><span class="comment"># 2. 缺失值可以用DMatrix构造函数中的默认值替换：</span></span><br><span class="line">dtrain = xgb.DMatrix(data, label=label, missing=<span class="number">-999.0</span>)</span><br><span class="line"><span class="comment"># 3.可以在需要时设置权重：</span></span><br><span class="line">w = np.random.rand(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">dtrain = xgb.DMatrix(data, label=label, missing=<span class="number">-999.0</span>, weight=w)</span><br></pre></td></tr></table></figure>
<h5 id="3-参数的设置方式"><a href="#3-参数的设置方式" class="headerlink" title="3). 参数的设置方式"></a>3). 参数的设置方式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载并处理数据</span></span><br><span class="line">df_wine = pd.read_csv(<span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'</span>,header=<span class="literal">None</span>)</span><br><span class="line">df_wine.columns = [<span class="string">'Class label'</span>, <span class="string">'Alcohol'</span>,<span class="string">'Malic acid'</span>, <span class="string">'Ash'</span>,<span class="string">'Alcalinity of ash'</span>,<span class="string">'Magnesium'</span>, <span class="string">'Total phenols'</span>,</span><br><span class="line">                   <span class="string">'Flavanoids'</span>, <span class="string">'Nonflavanoid phenols'</span>,<span class="string">'Proanthocyanins'</span>,<span class="string">'Color intensity'</span>, <span class="string">'Hue'</span>,<span class="string">'OD280/OD315 of diluted wines'</span>,<span class="string">'Proline'</span>]</span><br><span class="line">df_wine = df_wine[df_wine[<span class="string">'Class label'</span>] != <span class="number">1</span>]  <span class="comment"># drop 1 class</span></span><br><span class="line">y = df_wine[<span class="string">'Class label'</span>].values</span><br><span class="line">X = df_wine[[<span class="string">'Alcohol'</span>,<span class="string">'OD280/OD315 of diluted wines'</span>]].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split  <span class="comment"># 切分训练集与测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder   <span class="comment"># 标签化分类变量</span></span><br><span class="line"></span><br><span class="line">le = LabelEncoder()</span><br><span class="line">y = le.fit_transform(y)</span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">1</span>,stratify=y)</span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, label=y_train)</span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Booster 参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'booster'</span>: <span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'multi:softmax'</span>,  <span class="comment"># 多分类的问题</span></span><br><span class="line">    <span class="string">'num_class'</span>: <span class="number">10</span>,               <span class="comment"># 类别数，与 multisoftmax 并用</span></span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.1</span>,                  <span class="comment"># 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">12</span>,               <span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">    <span class="string">'lambda'</span>: <span class="number">2</span>,                   <span class="comment"># 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。</span></span><br><span class="line">    <span class="string">'subsample'</span>: <span class="number">0.7</span>,              <span class="comment"># 随机采样训练样本</span></span><br><span class="line">    <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,       <span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">    <span class="string">'min_child_weight'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">1</span>,                   <span class="comment"># 设置成1则没有运行信息输出，最好是设置为0.? slient还是verbosity?</span></span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.007</span>,                  <span class="comment"># 如同学习率</span></span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">'nthread'</span>: <span class="number">4</span>,                  <span class="comment"># cpu 线程数</span></span><br><span class="line">    <span class="string">'eval_metric'</span>:<span class="string">'auc'</span></span><br><span class="line">&#125;</span><br><span class="line">plst = params.items()</span><br><span class="line"><span class="comment"># evallist = [(dtest, 'eval'), (dtrain, 'train')]   # 指定验证集</span></span><br></pre></td></tr></table></figure>
<p>XGBoost can use either a list of pairs or a dictionary to set <a href="https://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="noopener">parameters</a>. For instance:</p>
<ul>
<li><p>Booster parameters</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">param = &#123;<span class="string">'max_depth'</span>: <span class="number">2</span>, <span class="string">'eta'</span>: <span class="number">1</span>, <span class="string">'objective'</span>: <span class="string">'binary:logistic'</span>&#125;</span><br><span class="line">param[<span class="string">'nthread'</span>] = <span class="number">4</span></span><br><span class="line">param[<span class="string">'eval_metric'</span>] = <span class="string">'auc'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>You can also specify multiple eval metrics:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">param[<span class="string">'eval_metric'</span>] = [<span class="string">'auc'</span>, <span class="string">'ams@0'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># alternatively:</span></span><br><span class="line"><span class="comment"># plst = param.items()</span></span><br><span class="line"><span class="comment"># plst += [('eval_metric', 'ams@0')]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Specify validations set to watch performance</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evallist = [(dtest, <span class="string">'eval'</span>), (dtrain, <span class="string">'train'</span>)]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="4-训练"><a href="#4-训练" class="headerlink" title="4). 训练"></a>4). 训练</h5><p>注：貌似现在不需要<code>plst = param.items()</code>，直接传 <code>param</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.训练</span></span><br><span class="line">num_round = <span class="number">10</span></span><br><span class="line">bst = xgb.train(plst, dtrain, num_round)</span><br><span class="line"><span class="comment">#bst = xgb.train(plst, dtrain, num_round, evallist )</span></span><br></pre></td></tr></table></figure>
<h5 id="5-保存模型"><a href="#5-保存模型" class="headerlink" title="5). 保存模型"></a>5). 保存模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.保存模型</span></span><br><span class="line"><span class="comment"># After training, the model can be saved.</span></span><br><span class="line">bst.save_model(<span class="string">'0001.model'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The model and its feature map can also be dumped to a text file.</span></span><br><span class="line"><span class="comment"># dump model</span></span><br><span class="line">bst.dump_model(<span class="string">'dump.raw.txt'</span>)</span><br><span class="line"><span class="comment"># dump model with feature map</span></span><br><span class="line"><span class="comment">#bst.dump_model('dump.raw.txt', 'featmap.txt')</span></span><br></pre></td></tr></table></figure>
<h5 id="6-加载保存的模型"><a href="#6-加载保存的模型" class="headerlink" title="6) . 加载保存的模型"></a>6) . 加载保存的模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.加载保存的模型：</span></span><br><span class="line">bst = xgb.Booster(&#123;<span class="string">'nthread'</span>: <span class="number">4</span>&#125;)  <span class="comment"># init model</span></span><br><span class="line">bst.load_model(<span class="string">'0001.model'</span>)  <span class="comment"># load data</span></span><br></pre></td></tr></table></figure>
<h5 id="7-设置早停机制"><a href="#7-设置早停机制" class="headerlink" title="7). 设置早停机制"></a>7). 设置早停机制</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5.也可以设置早停机制（需要设置验证集）</span></span><br><span class="line">train(..., evals=evals, early_stopping_rounds=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>The model will train until the validation score stops improving. Validation error needs to decrease at least every <code>early_stopping_rounds</code> to continue training.</p>
<p>If early stopping occurs, the model will have three additional fields: <code>bst.best_score</code>, <code>bst.best_iteration</code> and <code>bst.best_ntree_limit</code>. Note that <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train" target="_blank" rel="noopener"><code>xgboost.train()</code></a> will return a model from the last iteration, not the best one.</p>
<p>This works with both metrics to minimize (RMSE, log loss, etc.) and to maximize (MAP, NDCG, AUC). Note that if you specify more than one evaluation metric the last one in <code>param[&#39;eval_metric&#39;]</code> is used for early stopping.</p>
<h5 id="8-预测"><a href="#8-预测" class="headerlink" title="8). 预测"></a>8). 预测</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6.预测</span></span><br><span class="line">ypred = bst.predict(dtest)</span><br></pre></td></tr></table></figure>
<p>If early stopping is enabled during training, you can get predictions from the best iteration with <code>bst.best_ntree_limit</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)</span><br></pre></td></tr></table></figure>
<h5 id="9-绘图"><a href="#9-绘图" class="headerlink" title="9). 绘图"></a>9). 绘图</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.绘制重要性</span></span><br><span class="line"><span class="comment"># This function requires matplotlib to be installed.</span></span><br><span class="line">xgb.plot_importance(bst)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.绘制输出树</span></span><br><span class="line"><span class="comment"># This function requires graphviz and matplotlib.</span></span><br><span class="line"><span class="comment">#xgb.plot_tree(bst, num_trees=2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.使用xgboost.to_graphviz()将目标树转换为graphviz</span></span><br><span class="line"><span class="comment"># When you use IPython, you can use the xgboost.to_graphviz() function, which converts the target tree to a graphviz instance. The graphviz instance is automatically rendered in IPython.</span></span><br><span class="line"><span class="comment">#xgb.to_graphviz(bst, num_trees=2)</span></span><br></pre></td></tr></table></figure>
<h4 id="6-实战案例"><a href="#6-实战案例" class="headerlink" title="6. 实战案例"></a>6. 实战案例</h4><h5 id="1-分类案例"><a href="#1-分类案例" class="headerlink" title="1). 分类案例"></a>1). 分类案例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score   <span class="comment"># 准确率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载样本数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1234565</span>) <span class="comment"># 数据集分割</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 算法参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'booster'</span>: <span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'multi:softmax'</span>,</span><br><span class="line">    <span class="string">'num_class'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">'lambda'</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">'subsample'</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">'colsample_bytree'</span>: <span class="number">0.75</span>,</span><br><span class="line">    <span class="string">'min_child_weight'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'nthread'</span>: <span class="number">4</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plst = params.items()</span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, y_train) <span class="comment"># 生成数据集格式</span></span><br><span class="line">num_rounds = <span class="number">500</span></span><br><span class="line">model = xgb.train(plst, dtrain, num_rounds) <span class="comment"># xgboost模型训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测</span></span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line">y_pred = model.predict(dtest)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test,y_pred)</span><br><span class="line">print(<span class="string">"accuarcy: %.2f%%"</span> % (accuracy*<span class="number">100.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示重要特征</span></span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuarcy: 96.67%</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/25/决策树/1595830272543.png" alt="1595830272543"></p>
<h5 id="2-回归案例"><a href="#2-回归案例" class="headerlink" title="2). 回归案例"></a>2). 回归案例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">boston = load_boston()</span><br><span class="line">X,y = boston.data,boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># XGBoost训练过程</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'booster'</span>: <span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'reg:squarederror'</span>,</span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">'lambda'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'subsample'</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">'min_child_weight'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">'nthread'</span>: <span class="number">4</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dtrain = xgb.DMatrix(X_train, y_train)</span><br><span class="line">num_rounds = <span class="number">300</span></span><br><span class="line">plst = params.items()</span><br><span class="line">model = xgb.train(plst, dtrain, num_rounds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测</span></span><br><span class="line">dtest = xgb.DMatrix(X_test)</span><br><span class="line">ans = model.predict(dtest)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示重要特征</span></span><br><span class="line">plot_importance(model)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/07/25/决策树/1595837055748.png" alt="1595837055748"></p>
<h5 id="3-XGBoost调参（结合sklearn网格搜索）"><a href="#3-XGBoost调参（结合sklearn网格搜索）" class="headerlink" title="3). XGBoost调参（结合sklearn网格搜索）"></a><strong>3). XGBoost调参</strong>（结合sklearn网格搜索）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X,y = iris.data,iris.target</span><br><span class="line">col = iris.target_names</span><br><span class="line">train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)   <span class="comment"># 分训练集和验证集</span></span><br><span class="line"></span><br><span class="line">parameters = &#123;</span><br><span class="line">              <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>],</span><br><span class="line">              <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.15</span>],</span><br><span class="line">              <span class="string">'n_estimators'</span>: [<span class="number">500</span>, <span class="number">1000</span>, <span class="number">2000</span>, <span class="number">3000</span>, <span class="number">5000</span>],</span><br><span class="line">              <span class="string">'min_child_weight'</span>: [<span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">              <span class="string">'max_delta_step'</span>: [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">              <span class="string">'subsample'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.95</span>],</span><br><span class="line">              <span class="string">'colsample_bytree'</span>: [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>],</span><br><span class="line">              <span class="string">'reg_alpha'</span>: [<span class="number">0</span>, <span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>, <span class="number">1</span>],</span><br><span class="line">              <span class="string">'reg_lambda'</span>: [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>],</span><br><span class="line">              <span class="string">'scale_pos_weight'</span>: [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">xlf = xgb.XGBClassifier(max_depth=<span class="number">10</span>,</span><br><span class="line">            learning_rate=<span class="number">0.01</span>,</span><br><span class="line">            n_estimators=<span class="number">2000</span>,</span><br><span class="line">            silent=<span class="literal">True</span>,</span><br><span class="line">            objective=<span class="string">'multi:softmax'</span>,</span><br><span class="line">            num_class=<span class="number">3</span> ,</span><br><span class="line">            nthread=<span class="number">-1</span>,</span><br><span class="line">            gamma=<span class="number">0</span>,</span><br><span class="line">            min_child_weight=<span class="number">1</span>,</span><br><span class="line">            max_delta_step=<span class="number">0</span>,</span><br><span class="line">            subsample=<span class="number">0.85</span>,</span><br><span class="line">            colsample_bytree=<span class="number">0.7</span>,</span><br><span class="line">            colsample_bylevel=<span class="number">1</span>,</span><br><span class="line">            reg_alpha=<span class="number">0</span>,</span><br><span class="line">            reg_lambda=<span class="number">1</span>,</span><br><span class="line">            scale_pos_weight=<span class="number">1</span>,</span><br><span class="line">            seed=<span class="number">0</span>,</span><br><span class="line">            missing=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">gs = GridSearchCV(xlf, param_grid=parameters, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">3</span>)</span><br><span class="line">gs.fit(train_x, train_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best score: %0.3f"</span> % gs.best_score_)</span><br><span class="line">print(<span class="string">"Best parameters set: %s"</span> % gs.best_params_ )</span><br></pre></td></tr></table></figure>
<h2 id="决策树算法十问及经典面试问题"><a href="#决策树算法十问及经典面试问题" class="headerlink" title="决策树算法十问及经典面试问题"></a>决策树算法十问及经典面试问题</h2><p>（见 <a href="https://mp.weixin.qq.com/s/vkbZweJ5oRo4IPt-3kg64g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/vkbZweJ5oRo4IPt-3kg64g</a> ）</p>
<h3 id="简介和算法"><a href="#简介和算法" class="headerlink" title="简介和算法"></a>简介和算法</h3><blockquote>
<p>决策树是机器学习最常用的算法之一，它将算法组织成一颗树的形式。其实这就是将平时所说的if-then语句构建成了树的形式。这个决策树主要包括三个部分：内部节点、叶节点和边。内部节点是划分的属性，边代表划分的条件，叶节点表示类别。构建决策树 就是一个递归的选择内部节点，计算划分条件的边，最后到达叶子节点的过程。</p>
</blockquote>
<p><img src="/2020/07/25/决策树/1595669193083.png" alt="1595669193083"></p>
<h3 id="核心公式"><a href="#核心公式" class="headerlink" title="核心公式"></a>核心公式</h3><p><img src="/2020/07/25/决策树/1595669223727.png" alt="1595669223727"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">model</th>
<th style="text-align:center">feature select</th>
<th style="text-align:center">树的类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ID3</td>
<td style="text-align:center">{分类:信息增益}</td>
<td style="text-align:center">多叉树</td>
</tr>
<tr>
<td style="text-align:center">C4.5</td>
<td style="text-align:center">{分类:信息增益率}</td>
<td style="text-align:center">多叉树</td>
</tr>
<tr>
<td style="text-align:center">CART (分类树)</td>
<td style="text-align:center">{分类:基尼指数}</td>
<td style="text-align:center">二叉树</td>
</tr>
<tr>
<td style="text-align:center">CART (回归树)</td>
<td style="text-align:center">{回归:平方误差}</td>
<td style="text-align:center">二叉树</td>
</tr>
</tbody>
</table>
</div>
<h3 id="算法十问"><a href="#算法十问" class="headerlink" title="算法十问"></a>算法十问</h3><p>1.决策树和条件概率分布的关系？</p>
<blockquote>
<p>决策树可以表示成给定条件下类的条件概率分布. 决策树中的每一条路径都对应是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大.</p>
</blockquote>
<p>2.ID3和C4.5算法可以处理实数特征吗？如果可以应该怎么处理？如果不可以请给出理由？</p>
<p><img src="/2020/07/25/决策树/1595669343135.png" alt="1595669343135"></p>
<p>3.既然信息增益可以计算，为什么C4.5还使用信息增益比？</p>
<blockquote>
<p>在使用信息增益的时候，如果某个特征有很多取值，使用这个取值多的特征会的大的信息增益，这个问题是出现很多分支，将数据划分更细，模型复杂度高，出现过拟合的机率更大。使用信息增益比就是为了解决偏向于选择取值较多的特征的问题. 使用信息增益比对取值多的特征加上的惩罚，对这个问题进行了校正.</p>
</blockquote>
<p>4.基尼指数可以表示数据不确定性，信息熵也可以表示数据的不确定性. 为什么CART使用基尼指数？</p>
<blockquote>
<p>信息熵0, logK都是值越大，数据的不确定性越大. 信息熵需要计算对数，计算量大；信息熵是可以处理多个类别，基尼指数就是针对两个类计算的，由于CART树是一个二叉树，每次都是选择yes or no进行划分，从这个角度也是应该选择简单的基尼指数进行计算.</p>
</blockquote>
<p>5.决策树怎么剪枝？</p>
<blockquote>
<p>一般算法在构造决策树的都是尽可能的细分，直到数据不可划分才会到达叶子节点，停止划分. 因为给训练数据巨大的信任，这种形式形式很容易造成过拟合，为了防止过拟合需要进行决策树剪枝. 一般分为预剪枝和后剪枝，预剪枝是在决策树的构建过程中加入限制，比如控制叶子节点最少的样本个数，提前停止. 后剪枝是在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒.</p>
</blockquote>
<p>6.ID3算法，为什么不选择具有最高预测精度的属性特征，而是使用信息增益？</p>
<p>7.为什么使用贪心和其发生搜索建立决策树，为什么不直接使用暴力搜索建立最优的决策树？</p>
<blockquote>
<p>决策树目的是构建一个与训练数据拟合很好，并且复杂度小的决策树. 因为从所有可能的决策树中直接选择最优的决策树是NP完全问题，在使用中一般使用启发式方法学习相对最优的决策树.</p>
</blockquote>
<p>8.如果特征很多，决策树中最后没有用到的特征一定是无用吗？</p>
<blockquote>
<p>不是无用的，从两个角度考虑，一是特征替代性，如果可以已经使用的特征A和特征B可以提点特征C，特征C可能就没有被使用，但是如果把特征C单独拿出来进行训练，依然有效. 其二，决策树的每一条路径就是计算条件概率的条件，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据。</p>
</blockquote>
<p>9.决策树的优点？</p>
<blockquote>
<p>优点: 决策树模型可读性好，具有描述性，有助于人工分析；效率高，决策树只需要一次性构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。缺点: 对中间值的缺失敏感；可能产生过度匹配的问题，即过拟合。</p>
</blockquote>
<p>10.基尼系数存在的问题?</p>
<blockquote>
<p>基尼指数偏向于多值属性;当类数较大时，基尼指数求解比较困难;基尼指数倾向于支持在两个分区中生成大小相同的测试。</p>
</blockquote>
<h3 id="面试真题"><a href="#面试真题" class="headerlink" title="面试真题"></a>面试真题</h3><ol>
<li>决策树如何防止过拟合？</li>
<li>信息增益比相对信息增益有什么好处？</li>
<li>如果由异常值或者数据分布不均匀，会对决策树有什么影响？</li>
<li>手动构建CART的回归树的前两个节点，给出公式每一步的公式推到？</li>
<li>决策树和其他模型相比有什么优点？</li>
<li>决策树的目标函数是什么？</li>
</ol>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/04/02/Time-Series/" rel="next" title="Time Series">
                <i class="fa fa-chevron-left"></i> Time Series
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/07/28/深度学习相关资料/" rel="prev" title="深度学习相关资料">
                深度学习相关资料 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="qypx">
            
              <p class="site-author-name" itemprop="name">qypx</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">98</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">46</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qypx" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          


          
          

          
          

          


          <!-- 新增的内容 -->
          <!-- require APlayer -->
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
          <script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script>
          <!-- require MetingJS -->
          <script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

          <meting-js server="netease" type="playlist" id="4870130923" list-folded="true" order="random">
          </meting-js>
          <!-- 新增的内容end -->

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#篇一：决策树-（ID3-C4-5-CART）"><span class="nav-text">篇一：决策树 （ID3, C4.5, CART）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3"><span class="nav-text">ID3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-思想"><span class="nav-text">1.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-划分标准"><span class="nav-text">1.2 划分标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-缺点"><span class="nav-text">1.3 缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C4-5"><span class="nav-text">C4.5</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-思想"><span class="nav-text">2.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-划分标准"><span class="nav-text">2.2 划分标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-剪枝策略"><span class="nav-text">2.3 剪枝策略</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-1-预剪枝"><span class="nav-text">2.3.1 预剪枝</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-2-后剪枝"><span class="nav-text">2.3.2 后剪枝</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-缺点"><span class="nav-text">2.4 缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART"><span class="nav-text">CART</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-思想"><span class="nav-text">3.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-划分标准"><span class="nav-text">3.2 划分标准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-缺失值处理"><span class="nav-text">3.3 缺失值处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-剪枝策略"><span class="nav-text">3.4 剪枝策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-类别不平衡"><span class="nav-text">3.5 类别不平衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-回归树"><span class="nav-text">3.6 回归树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-6-1-连续值处理"><span class="nav-text">3.6.1 连续值处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-6-2-预测方式"><span class="nav-text">3.6.2 预测方式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#篇二：Random-Forest-Adaboost-GBDT算法"><span class="nav-text">篇二：Random Forest, Adaboost, GBDT算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#集成学习"><span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-Bagging"><span class="nav-text">1.1 Bagging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-Boosting"><span class="nav-text">1.2 Boosting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-Stacking"><span class="nav-text">1.3 Stacking</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#偏差与方差"><span class="nav-text">偏差与方差</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-集成学习的偏差与方差"><span class="nav-text">2.1 集成学习的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Bagging-的偏差与方差"><span class="nav-text">2.2 Bagging 的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Boosting-的偏差与方差"><span class="nav-text">2.3 Boosting 的偏差与方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-小结"><span class="nav-text">2.4 小结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Random-Forest"><span class="nav-text">Random Forest</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-思想-1"><span class="nav-text">3.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-优缺点"><span class="nav-text">3.2 优缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost"><span class="nav-text">AdaBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-思想"><span class="nav-text">4.1 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-细节"><span class="nav-text">4.2 细节</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-1-损失函数"><span class="nav-text">4.2.1 损失函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-正则化"><span class="nav-text">4.2.2 正则化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-优缺点"><span class="nav-text">4.3 优缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-text">GBDT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-思想"><span class="nav-text">5.1 思想</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-1-回归树（Regression-Decision-Tree）"><span class="nav-text">5.1.1 回归树（Regression Decision Tree）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-2-梯度迭代（Gradient-Boosting）"><span class="nav-text">5.1.2 梯度迭代（Gradient Boosting）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-13-缩减（Shrinkage）"><span class="nav-text">5.1.13 缩减（Shrinkage）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-优缺点"><span class="nav-text">5.2 优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-与-Adaboost-的对比"><span class="nav-text">5.3 与 Adaboost 的对比</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#篇三：XGBoost"><span class="nav-text">篇三：XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost"><span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-数学原理"><span class="nav-text">1.1 数学原理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-1-目标函数"><span class="nav-text">1.1.1 目标函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-2-基于决策树的目标函数"><span class="nav-text">1.1.2 基于决策树的目标函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-3-最优切分点划分算法"><span class="nav-text">1.1.3 最优切分点划分算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1）贪心算法-精确贪心算法-Basic-Exact-Greedy-Algorithm"><span class="nav-text">1）贪心算法 (精确贪心算法 Basic Exact Greedy Algorithm)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2）近似算法-Approximate-Algorithm"><span class="nav-text">2）近似算法 (Approximate Algorithm)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-4-加权分位数缩略图"><span class="nav-text">1.1.4 加权分位数缩略图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-5-稀疏感知算法"><span class="nav-text">1.1.5 稀疏感知算法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-工程实现"><span class="nav-text">1.2 工程实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-1-块结构设计"><span class="nav-text">1.2.1 块结构设计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-2-缓存访问优化算法"><span class="nav-text">1.2.2 缓存访问优化算法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-3-“核外”块计算"><span class="nav-text">1.2.3 “核外”块计算</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-优缺点"><span class="nav-text">1.3 优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-1-优点"><span class="nav-text">1.3.1 优点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-2-缺点"><span class="nav-text">1.3.2 缺点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost动手实践"><span class="nav-text">XGBoost动手实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-引入基本工具库"><span class="nav-text">1. 引入基本工具库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-XGBoost原生工具库的上手"><span class="nav-text">2. XGBoost原生工具库的上手</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-XGBoost的参数设置"><span class="nav-text">3. XGBoost的参数设置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-通用参数"><span class="nav-text">3.1. 通用参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-任务参数"><span class="nav-text">3.2. 任务参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-命令行参数"><span class="nav-text">3.3. 命令行参数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-XGBoost的调参说明"><span class="nav-text">4. XGBoost的调参说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-XGBoost详细攻略"><span class="nav-text">5. XGBoost详细攻略</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-安装XGBoost"><span class="nav-text">1). 安装XGBoost</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-数据接口（XGBoost可处理的数据格式DMatrix）"><span class="nav-text">2). 数据接口（XGBoost可处理的数据格式DMatrix）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-参数的设置方式"><span class="nav-text">3). 参数的设置方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-训练"><span class="nav-text">4). 训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-保存模型"><span class="nav-text">5). 保存模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-加载保存的模型"><span class="nav-text">6) . 加载保存的模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-设置早停机制"><span class="nav-text">7). 设置早停机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-预测"><span class="nav-text">8). 预测</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#9-绘图"><span class="nav-text">9). 绘图</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-实战案例"><span class="nav-text">6. 实战案例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-分类案例"><span class="nav-text">1). 分类案例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-回归案例"><span class="nav-text">2). 回归案例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-XGBoost调参（结合sklearn网格搜索）"><span class="nav-text">3). XGBoost调参（结合sklearn网格搜索）</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树算法十问及经典面试问题"><span class="nav-text">决策树算法十问及经典面试问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介和算法"><span class="nav-text">简介和算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核心公式"><span class="nav-text">核心公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法十问"><span class="nav-text">算法十问</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面试真题"><span class="nav-text">面试真题</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qypx</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left"},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
