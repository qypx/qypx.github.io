<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Computer Vision,">










<meta name="description" content="来源于Here’s your Learning Path to Master Computer Vision in 2020)">
<meta name="keywords" content="Computer Vision">
<meta property="og:type" content="article">
<meta property="og:title" content="Computer Vision">
<meta property="og:url" content="http://qypx.github.io/2020/02/17/Computer-Vision/index.html">
<meta property="og:site_name" content="qypx の blog">
<meta property="og:description" content="来源于Here’s your Learning Path to Master Computer Vision in 2020)">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://qypx.github.io/2020/02/17/Computer-Vision/1581936373011.png">
<meta property="og:image" content="http://qypx.github.io/2020/02/17/Computer-Vision/1581936472914.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-16.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-41.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_1.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_2-300x289.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-52.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_3.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-6.png">
<meta property="og:image" content="http://qypx.github.io/2020/02/17/Computer-Vision/article-image-71.webp">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-81.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-9.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-101.png">
<meta property="og:image" content="http://qypx.github.io/2020/02/17/Computer-Vision/article-image-132-300x159.webp">
<meta property="og:image" content="http://qypx.github.io/2020/02/17/Computer-Vision/article-image-111.webp">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-121.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_61.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-1.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-2.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_7.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_8.png">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_9.png">
<meta property="og:updated_time" content="2020-02-18T06:29:26.308Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Computer Vision">
<meta name="twitter:description" content="来源于Here’s your Learning Path to Master Computer Vision in 2020)">
<meta name="twitter:image" content="http://qypx.github.io/2020/02/17/Computer-Vision/1581936373011.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://qypx.github.io/2020/02/17/Computer-Vision/">





<!-- 网页加载条 -->
<script src="https://neveryu.github.io/js/src/pace.min.js"></script>

  <title>Computer Vision | qypx の blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">qypx の blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">机会是留给有准备的人的.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://qypx.github.io/2020/02/17/Computer-Vision/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="qypx">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="qypx の blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Computer Vision</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-17T18:39:54+08:00">
                2020-02-17
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2020-02-18T14:29:26+08:00">
                2020-02-18
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Vision/" itemprop="url" rel="index">
                    <span itemprop="name">Computer Vision</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>来源于<a href="https://www.analyticsvidhya.com/blog/2020/01/computer-vision-learning-path-2020/?utm_source=feedburner&amp;utm_medium=email&amp;utm_campaign=Feed%3A+AnalyticsVidhya+(Analytics+Vidhya" target="_blank" rel="noopener">Here’s your Learning Path to Master Computer Vision in 2020</a>)</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="Face-Recognition"><a href="#Face-Recognition" class="headerlink" title="Face Recognition"></a>Face Recognition</h3><p><img src="/2020/02/17/Computer-Vision/1581936373011.png" alt="1581936373011"></p>
<h3 id="Types-of-computer-vision"><a href="#Types-of-computer-vision" class="headerlink" title="Types of computer vision"></a>Types of computer vision</h3><p><img src="/2020/02/17/Computer-Vision/1581936472914.png" alt="1581936472914"></p>
<h2 id="2-Image-Preprocessing"><a href="#2-Image-Preprocessing" class="headerlink" title="2. Image Preprocessing"></a>2. Image Preprocessing</h2><h3 id="2-1-Three-Beginner-Friendly-Techniques-to-Extract-Features-from-Images"><a href="#2-1-Three-Beginner-Friendly-Techniques-to-Extract-Features-from-Images" class="headerlink" title="2.1 Three Beginner-Friendly Techniques to Extract Features from Images"></a>2.1 Three Beginner-Friendly Techniques to Extract Features from Images</h3><p><a href="https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/?utm_source=blog&amp;utm_medium=computer-vision-learning-path-2020" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/?utm_source=blog&amp;utm_medium=computer-vision-learning-path-2020</a></p>
<h4 id="2-1-1-How-do-Machines-Store-Images"><a href="#2-1-1-How-do-Machines-Store-Images" class="headerlink" title="2.1.1 How do Machines Store Images?"></a>2.1.1 How do Machines Store Images?</h4><p>Machines store images in the form of a matrix of numbers. The size of this matrix depends on the number of pixels we have in any given image.</p>
<blockquote>
<p><em>Let’s say the dimensions of an image are 180 x 200 or n x m. These dimensions are basically the number of pixels in the image (height x width).</em></p>
</blockquote>
<p><strong>These numbers, or the pixel values, denote the intensity or brightness of the pixel.</strong> <span style="color:red">Smaller numbers (closer to zero) represent black, and larger numbers (closer to 255) denote white. </span>You’ll understand whatever we have learned so far by analyzing the below image.</p>
<p>The dimensions of the below image are 22 x 16, which you can verify by counting the number of pixels:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-16.png" alt="reading image data machine learning"></p>
<blockquote>
<p><em>A colored image is typically composed of multiple colors and almost all colors can be generated from three primary colors – red, green and blue.</em></p>
</blockquote>
<p>In the case of a colored image, there are three Matrices (or channels) – Red, Green, and Blue. <strong>Each matrix has values between 0-255 representing the intensity of the color for that pixel.</strong> Consider the below image to understand this concept:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-41.png" alt="Channels in Images"></p>
<p>The three channels are superimposed (叠加) to form a colored image.</p>
<p><em>Note that these are not the original pixel values for the given image as the original matrix would be very large and difficult to visualize. Also, there are various other formats in which the images are stored. RGB is the most popular one and hence I have addressed it here. You can read more about the other popular formats <a href="https://www.w3schools.com/cssref/css_colors_legal.asp" target="_blank" rel="noopener">here</a>.</em></p>
<h4 id="2-1-2-Reading-Image-Data-in-Python"><a href="#2-1-2-Reading-Image-Data-in-Python" class="headerlink" title="2.1.2 Reading Image Data in Python"></a>2.1.2 Reading Image Data in Python</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> skimage.io <span class="keyword">import</span> imread, imshow</span><br><span class="line"></span><br><span class="line">image = imread(<span class="string">'image_8_original.png'</span>, as_gray=<span class="literal">True</span>)</span><br><span class="line">imshow(image)</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_1.png" alt="image data machine learning"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#checking image shape </span></span><br><span class="line">image.shape, image</span><br></pre></td></tr></table></figure>
<p>(28,28)</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_2-300x289.png" alt="image data machine learning"></p>
<p>Let’s now explore various methods of using pixel values as features.</p>
<h4 id="2-1-3-Method-1-Grayscale-Pixel-Values-as-Features"><a href="#2-1-3-Method-1-Grayscale-Pixel-Values-as-Features" class="headerlink" title="2.1.3 Method #1: Grayscale Pixel Values as Features"></a>2.1.3 Method #1: Grayscale Pixel Values as Features</h4><blockquote>
<p><em>The simplest way to create features from an image is to use these raw pixel values as separate features.</em></p>
</blockquote>
<p>The number of features will be the same as the number of pixels.</p>
<p>How do we arrange these pixels as features? Well, we can simply append every pixel value one after the other to generate a feature vector. This is illustrated in the image below:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-52.png" alt="pixel features machine learning"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image = imread(<span class="string">'puppy.jpeg'</span>, as_gray=<span class="literal">True</span>) </span><br><span class="line">image.shape, imshow(image)</span><br></pre></td></tr></table></figure>
<p>(660, 450)</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_3.png" alt="image data machine learning"></p>
<p>The image shape here is 660 x 450. Hence, the number of features should be 297,000. We can generate this using the <code>reshape</code> function from NumPy where we specify the dimension of the image:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pixel features</span></span><br><span class="line">features = np.reshape(image, (<span class="number">660</span>*<span class="number">450</span>))</span><br><span class="line">features.shape, features</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(297000,)</span><br><span class="line">array([0.96470588, 0.96470588, 0.96470588, ..., 0.96862745, 0.96470588,</span><br><span class="line">       0.96470588])</span><br></pre></td></tr></table></figure>
<p>But here, we only had a single channel or a grayscale image. Can we do the same for a colored image? Let’s find out!</p>
<h4 id="2-1-4-Method-2-Mean-Pixel-Value-of-Channels-Colored-Image"><a href="#2-1-4-Method-2-Mean-Pixel-Value-of-Channels-Colored-Image" class="headerlink" title="2.1.4 Method #2: Mean Pixel Value of Channels (Colored Image)"></a>2.1.4 Method #2: Mean Pixel Value of Channels (Colored Image)</h4><p>While reading the image in the previous section, we had set the parameter <em>‘as_gray = True’</em>. So we only had one channel in the image and we could easily append the pixel values. Let us remove the parameter and load the image again:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image = imread(<span class="string">'puppy.jpeg'</span>) </span><br><span class="line">image.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(660, 450, 3)</span><br></pre></td></tr></table></figure>
<p>This time, the image has a dimension (660, 450, 3), where 3 is the number of channels. We can go ahead and create the features as we did previously. The number of features, in this case, will be 660*450*3 = 891,000.</p>
<p>Alternatively, here is another approach we can use:</p>
<blockquote>
<p><em>Instead of using the pixel values from the three channels separately, we can generate a new matrix that has the mean value of pixels from all three channels.</em></p>
</blockquote>
<p>The image below will give you even more clarity around this idea:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-6.png" alt="image pixel features"></p>
<p>By doing so, the number of features remains the same and we also take into account the pixel values from all three channels of the image. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">image = imread(<span class="string">'puppy.jpeg'</span>)</span><br><span class="line">feature_matrix = np.zeros((<span class="number">660</span>,<span class="number">450</span>)) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,image.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,image.shape[<span class="number">1</span>]):</span><br><span class="line">        feature_matrix[i][j] = ((int(image[i,j,<span class="number">0</span>]) + int(image[i,j,<span class="number">1</span>]) + int(image[i,j,<span class="number">2</span>]))/<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>The new matrix will have the same height and width but only 1 channel. Now we can follow the same steps that we did in the previous section. We append the pixel values one after the other to get a 1D array:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">features = np.reshape(feature_matrix, (<span class="number">660</span>*<span class="number">450</span>)) </span><br><span class="line">features.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(297000,)</span><br></pre></td></tr></table></figure>
<h4 id="2-1-5-Method-3-Extracting-Edge-Features"><a href="#2-1-5-Method-3-Extracting-Edge-Features" class="headerlink" title="2.1.5 Method #3: Extracting Edge Features"></a>2.1.5 Method #3: Extracting Edge Features</h4><p>Consider that we are given the below image and we need to identify the objects present in it:</p>
<p><img src="/2020/02/17/Computer-Vision/article-image-71.webp" alt="article-image-71"></p>
<p>You must have recognized the objects in an instant – a dog, a car and a cat. What are the features that you considered while differentiating each of these images? The shape could be one important factor, followed by color, or size. What if the machine could also identify the shape as we do?</p>
<p>A similar idea is to <strong>extract edges as features and use that as the input for the model.</strong> I want you to think about this for a moment – how can we identify edges in an image? <strong>Edge is basically where there is a sharp change in color</strong>. Look at the below image:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-81.png" alt="edge features"></p>
<p>And as we know, an image is represented in the form of numbers. So, we will look for pixels around which there is a drastic change in the pixel values.</p>
<p>Let’s say we have the following matrix for the image:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-9.png" alt="img"></p>
<p>To identify if a pixel is an edge or not, we will simply subtract the values on either side of the pixel. For this example, we have the highlighted value of 85. We will find the difference between the values 89 and 78. Since this difference is not very large, we can say that there is no edge around this pixel.</p>
<p>Now consider the pixel 125 highlighted in the below image:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-101.png" alt="img"></p>
<p>Since the difference between the values on either side of this pixel is large, we can conclude that there is a significant transition at this pixel and hence it is an edge. Now the question is, do we have to do this step manually?</p>
<p>No! <strong>There are various kernels that can be used to highlight the edges in an image.</strong> The method we just discussed can also be achieved using the Prewitt kernel (in the x-direction). Given below is the Prewitt kernel:</p>
<p><img src="/2020/02/17/Computer-Vision/article-image-132-300x159.webp" alt="article-image-132-300x159"></p>
<p>We take the values surrounding the selected pixel and multiply it with the selected kernel (Prewitt kernel). We can then add the resulting values to get a final value. Since we already have -1 in one column and 1 in the other column, adding the values is equivalent to taking the difference.</p>
<p><img src="/2020/02/17/Computer-Vision/article-image-111.webp" alt="article-image-111"></p>
<p>There are various other kernels and I have mentioned four most popularly used ones below:</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-121.png" alt="kernels"></p>
<p>Let’s now go back to the notebook and generate edge features for the same image:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#importing the required libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skimage.io <span class="keyword">import</span> imread, imshow</span><br><span class="line"><span class="keyword">from</span> skimage.filters <span class="keyword">import</span> prewitt_h,prewitt_v</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#reading the image </span></span><br><span class="line">image = imread(<span class="string">'puppy.jpeg'</span>,as_gray=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#calculating horizontal edges using prewitt kernel</span></span><br><span class="line">edges_prewitt_horizontal = prewitt_h(image)</span><br><span class="line"><span class="comment">#calculating vertical edges using prewitt kernel</span></span><br><span class="line">edges_prewitt_vertical = prewitt_v(image)</span><br><span class="line"></span><br><span class="line">imshow(edges_prewitt_vertical, cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_61.png" alt="edge features"></p>
<h3 id="2-2-HOG-Histogram-of-Oriented-Gradients-features"><a href="#2-2-HOG-Histogram-of-Oriented-Gradients-features" class="headerlink" title="2.2 HOG (Histogram of Oriented Gradients) features"></a>2.2 HOG (Histogram of Oriented Gradients) features</h3><p><a href="https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/?utm_source=blog&amp;utm_medium=computer-vision-learning-path-2020" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/?utm_source=blog&amp;utm_medium=computer-vision-learning-path-2020</a></p>
<h4 id="2-2-1-What-is-a-Feature-Descriptor"><a href="#2-2-1-What-is-a-Feature-Descriptor" class="headerlink" title="2.2.1 What is a Feature Descriptor?"></a>2.2.1 What is a Feature Descriptor?</h4><p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-1.png" alt="HOG feature"></p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/article-image-2.png" alt="HOG feature"></p>
<p>The first pair of images had a lot of information, like the shape of the object, its color, the edges, background, etc.</p>
<p>On the other hand, the second pair had much less information (only the shape and the edges) but it was still enough to differentiate the two images.</p>
<p>We were easily able to differentiate the objects in the second case because it had the necessary information we would need to identify the object. And that is exactly what a <strong>feature descriptor</strong> does:</p>
<blockquote>
<p><em>It is a simplified representation of the image that contains only the most important information about the image.</em></p>
</blockquote>
<p>There are a number of feature descriptors out there. Here are a few of the most popular ones:</p>
<ul>
<li>HOG: Histogram of Oriented Gradients</li>
<li>SIFT: Scale Invariant Feature Transform</li>
<li>SURF: Speeded-Up Robust Feature</li>
</ul>
<h4 id="2-2-2-Introduction-to-the-HOG-Feature-Descriptor"><a href="#2-2-2-Introduction-to-the-HOG-Feature-Descriptor" class="headerlink" title="2.2.2 Introduction to the HOG Feature Descriptor"></a>2.2.2 Introduction to the HOG Feature Descriptor</h4><p>HOG, or Histogram of Oriented Gradients, is a feature descriptor that is often used to extract features from image data. It is widely used in <a href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2/?utm_source=blog&amp;utm_medium=understand-math-HOG-feature-descriptor" target="_blank" rel="noopener">computer vision</a> tasks for <a href="https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/?utm_source=blog&amp;utm_medium=understand-math-HOG-feature-descriptor" target="_blank" rel="noopener">object detection</a>.</p>
<p>Let’s look at some important aspects of HOG that makes it different from other feature descriptors:</p>
<ul>
<li>The HOG descriptor focuses on the <strong>structure or the shape</strong> of an object. Now you might ask, how is this different from the edge features we extract for images? In the case of edge features, we only identify if the pixel is an edge or not. HOG is able to provide the edge direction as well. This is done by extracting the <strong>gradient and orientation</strong> (or you can say magnitude and direction) of the edges</li>
<li>Additionally, these orientations are calculated in <strong>‘localized’ portions</strong>. This means that the complete image is broken down into smaller regions and for each region, the gradients and orientation are calculated. We will discuss this in much more detail in the upcoming sections</li>
<li>Finally the HOG would generate a <strong>Histogram</strong> for each of these regions separately. The histograms are created using the gradients and orientations of the pixel values, hence the name ‘Histogram of Oriented Gradients’</li>
</ul>
<p>To put a formal definition to this:</p>
<blockquote>
<p>The HOG feature descriptor counts the occurrences of gradient orientation in localized portions of an image.</p>
</blockquote>
<p>Implementing HOG using tools like OpenCV is extremely simple. It’s just a few lines of code since we have a predefined function called <strong>hog</strong> in the <strong>skimage.feature</strong> library. Our focus in this article, however, is on how these features are actually calculated.</p>
<h4 id="2-2-3-Process-of-Calculating-the-Histogram-of-Oriented-Gradients-HOG"><a href="#2-2-3-Process-of-Calculating-the-Histogram-of-Oriented-Gradients-HOG" class="headerlink" title="2.2.3 Process of Calculating the Histogram of Oriented Gradients (HOG)"></a>2.2.3 Process of Calculating the Histogram of Oriented Gradients (HOG)</h4><p>HOG具体是怎么计算的：详细内容见<a href="https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/?utm_source=blog&amp;utm_medium=computer-vision-learning-path-2020" target="_blank" rel="noopener">这里</a>.</p>
<h4 id="2-2-4-Implementing-HOG-Feature-Descriptor-in-Python"><a href="#2-2-4-Implementing-HOG-Feature-Descriptor-in-Python" class="headerlink" title="2.2.4 Implementing HOG Feature Descriptor in Python"></a>2.2.4 Implementing HOG Feature Descriptor in Python</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#importing required libraries</span></span><br><span class="line"><span class="keyword">from</span> skimage.io <span class="keyword">import</span> imread, imshow</span><br><span class="line"><span class="keyword">from</span> skimage.transform <span class="keyword">import</span> resize</span><br><span class="line"><span class="keyword">from</span> skimage.feature <span class="keyword">import</span> hog</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> exposure</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#reading the image</span></span><br><span class="line">img = imread(<span class="string">'puppy.jpeg'</span>)</span><br><span class="line">imshow(img)</span><br><span class="line">print(img.shape)</span><br></pre></td></tr></table></figure>
<p>(663, 459, 3)</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_7.png" alt="hog_feature"></p>
<p>We can see that the shape of the image is 663 x 459. We will have to resize this image into 64 x 128. Note that we are using <code>skimage</code> which takes the input as height x width.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#resizing image </span></span><br><span class="line">resized_img = resize(img, (<span class="number">128</span>,<span class="number">64</span>)) </span><br><span class="line">imshow(resized_img) </span><br><span class="line">print(resized_img.shape)</span><br></pre></td></tr></table></figure>
<p>(128, 64, 3)</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_8.png" alt="hog_feature"></p>
<p>Here, I am going to use the hog function from <code>skimage.feature</code> directly. So we don’t have to calculate the gradients, magnitude (total gradient) and orientation individually. The hog function would internally calculate it and return the feature matrix.</p>
<p>Also, if you set the parameter <code>visualize = True</code>, it will return an image of the HOG.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#creating hog features </span></span><br><span class="line">fd, hog_image = hog(resized_img, orientations=<span class="number">9</span>, pixels_per_cell=(<span class="number">8</span>, <span class="number">8</span>), </span><br><span class="line">                    cells_per_block=(<span class="number">2</span>, <span class="number">2</span>), visualize=<span class="literal">True</span>, multichannel=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Before going ahead, let me give you a basic idea of what each of these hyperparameters represents. Alternatively, you can check the definitions from the official documentation <a href="https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.hog" target="_blank" rel="noopener">here</a>.</p>
<ul>
<li>The <code>orientations</code> are the number of buckets we want to create. Since I want to have a 9 x 1 matrix, I will set the orientations to 9</li>
<li><code>pixels_per_cell</code> defines the size of the cell for which we create the histograms. In the example we covered in this article, we used 8 x 8 cells and here I will set the same value. As mentioned previously, you can choose to change this value</li>
<li>We have another hyperparameter <code>cells_per_block</code> which is the size of the block over which we normalize the histogram. Here, we mention the cells per blocks and not the number of pixels. So, instead of writing 16 x 16, we will use 2 x 2 here</li>
</ul>
<p>The feature matrix from the function is stored in the variable <code>fd</code>, and the image is stored in <code>hog_image</code>. Let us check the shape of the feature matrix:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fd.shape</span><br></pre></td></tr></table></figure>
<p>(3780,)</p>
<p>As expected, we have 3,780 features for the image and this verifies the calculations we did in step 7 earlier. You can choose to change the values of the hyperparameters and that will give you a feature matrix of different sizes.</p>
<p>Let’s finally look at the HOG image:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">16</span>, <span class="number">8</span>), sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">ax1.imshow(resized_img, cmap=plt.cm.gray) </span><br><span class="line">ax1.set_title(<span class="string">'Input image'</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Rescale histogram for better display </span></span><br><span class="line">hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(<span class="number">0</span>, <span class="number">10</span>)) </span><br><span class="line"></span><br><span class="line">ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray) </span><br><span class="line">ax2.set_title(<span class="string">'Histogram of Oriented Gradients'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/index_9.png" alt="hog_features"></p>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/02/16/B树介绍/" rel="next" title="B树介绍">
                <i class="fa fa-chevron-left"></i> B树介绍
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/17/hexo相关问题/" rel="prev" title="hexo相关问题">
                hexo相关问题 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="qypx">
            
              <p class="site-author-name" itemprop="name">qypx</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">90</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">42</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/qypx" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          


          
          

          
          

          


          <!-- 新增的内容 -->
          <!-- require APlayer -->
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css">
          <script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script>
          <!-- require MetingJS -->
          <script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

          <meting-js server="netease" type="playlist" id="4870130923" list-folded="true" order="random">
          </meting-js>
          <!-- 新增的内容end -->

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Face-Recognition"><span class="nav-text">Face Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Types-of-computer-vision"><span class="nav-text">Types of computer vision</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Image-Preprocessing"><span class="nav-text">2. Image Preprocessing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Three-Beginner-Friendly-Techniques-to-Extract-Features-from-Images"><span class="nav-text">2.1 Three Beginner-Friendly Techniques to Extract Features from Images</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-How-do-Machines-Store-Images"><span class="nav-text">2.1.1 How do Machines Store Images?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-Reading-Image-Data-in-Python"><span class="nav-text">2.1.2 Reading Image Data in Python</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-Method-1-Grayscale-Pixel-Values-as-Features"><span class="nav-text">2.1.3 Method #1: Grayscale Pixel Values as Features</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-4-Method-2-Mean-Pixel-Value-of-Channels-Colored-Image"><span class="nav-text">2.1.4 Method #2: Mean Pixel Value of Channels (Colored Image)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-5-Method-3-Extracting-Edge-Features"><span class="nav-text">2.1.5 Method #3: Extracting Edge Features</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-HOG-Histogram-of-Oriented-Gradients-features"><span class="nav-text">2.2 HOG (Histogram of Oriented Gradients) features</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-What-is-a-Feature-Descriptor"><span class="nav-text">2.2.1 What is a Feature Descriptor?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-Introduction-to-the-HOG-Feature-Descriptor"><span class="nav-text">2.2.2 Introduction to the HOG Feature Descriptor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-Process-of-Calculating-the-Histogram-of-Oriented-Gradients-HOG"><span class="nav-text">2.2.3 Process of Calculating the Histogram of Oriented Gradients (HOG)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-Implementing-HOG-Feature-Descriptor-in-Python"><span class="nav-text">2.2.4 Implementing HOG Feature Descriptor in Python</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">qypx</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
